{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tsundoku\n",
      "  Downloading https://files.pythonhosted.org/packages/53/9a/ee69fb151e5c628683f67021c39d327a24a2f0ae81d4c433823711756a11/tsundoku-0.0.3.tar.gz\n",
      "Requirement already satisfied: IPython in c:\\users\\monis\\anaconda3\\lib\\site-packages (from tsundoku)\n",
      "Requirement already satisfied: torch in c:\\users\\monis\\anaconda3\\lib\\site-packages (from tsundoku)\n",
      "Requirement already satisfied: nltk in c:\\users\\monis\\anaconda3\\lib\\site-packages (from tsundoku)\n",
      "Requirement already satisfied: gensim in c:\\users\\monis\\anaconda3\\lib\\site-packages (from tsundoku)\n",
      "Requirement already satisfied: tqdm in c:\\users\\monis\\anaconda3\\lib\\site-packages (from tsundoku)\n",
      "Requirement already satisfied: numpy in c:\\users\\monis\\anaconda3\\lib\\site-packages (from tsundoku)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from IPython->tsundoku)\n",
      "Requirement already satisfied: jedi>=0.10 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from IPython->tsundoku)\n",
      "Requirement already satisfied: decorator in c:\\users\\monis\\anaconda3\\lib\\site-packages (from IPython->tsundoku)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\monis\\anaconda3\\lib\\site-packages (from IPython->tsundoku)\n",
      "Requirement already satisfied: simplegeneric>0.8 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from IPython->tsundoku)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from IPython->tsundoku)\n",
      "Requirement already satisfied: prompt_toolkit<2.0.0,>=1.0.4 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from IPython->tsundoku)\n",
      "Requirement already satisfied: pygments in c:\\users\\monis\\anaconda3\\lib\\site-packages (from IPython->tsundoku)\n",
      "Requirement already satisfied: colorama in c:\\users\\monis\\anaconda3\\lib\\site-packages (from IPython->tsundoku)\n",
      "Requirement already satisfied: six in c:\\users\\monis\\anaconda3\\lib\\site-packages (from nltk->tsundoku)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from gensim->tsundoku)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from gensim->tsundoku)\n",
      "Requirement already satisfied: parso==0.1.* in c:\\users\\monis\\anaconda3\\lib\\site-packages (from jedi>=0.10->IPython->tsundoku)\n",
      "Requirement already satisfied: ipython_genutils in c:\\users\\monis\\anaconda3\\lib\\site-packages (from traitlets>=4.2->IPython->tsundoku)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\monis\\anaconda3\\lib\\site-packages (from prompt_toolkit<2.0.0,>=1.0.4->IPython->tsundoku)\n",
      "Requirement already satisfied: requests in c:\\users\\monis\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim->tsundoku)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim->tsundoku)\n",
      "Requirement already satisfied: boto3 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim->tsundoku)\n",
      "Requirement already satisfied: bz2file in c:\\users\\monis\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim->tsundoku)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim->tsundoku)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim->tsundoku)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim->tsundoku)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.2.1->gensim->tsundoku)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim->tsundoku)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.7 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim->tsundoku)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim->tsundoku)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.7->boto3->smart-open>=1.2.1->gensim->tsundoku)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\users\\monis\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.7->boto3->smart-open>=1.2.1->gensim->tsundoku)\n",
      "Building wheels for collected packages: tsundoku\n",
      "  Running setup.py bdist_wheel for tsundoku: started\n",
      "  Running setup.py bdist_wheel for tsundoku: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\monis\\AppData\\Local\\pip\\Cache\\wheels\\ee\\b3\\9f\\bfdb5e507142dacffd199c4aaa9c58d805191f24e4d7d71905\n",
      "Successfully built tsundoku\n",
      "Installing collected packages: tsundoku\n",
      "Successfully installed tsundoku-0.0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 19.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tsundoku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sklearn in c:\\users\\monis\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: torch in c:\\users\\monis\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: tqdm in c:\\users\\monis\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: nltk in c:\\users\\monis\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: lazyme in c:\\users\\monis\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: requests in c:\\users\\monis\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: gensim in c:\\users\\monis\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\monis\\anaconda3\\lib\\site-packages (from sklearn)\n",
      "Requirement already satisfied: six in c:\\users\\monis\\anaconda3\\lib\\site-packages (from nltk)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from requests)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from gensim)\n",
      "Requirement already satisfied: boto>=2.32 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: boto3 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: bz2file in c:\\users\\monis\\anaconda3\\lib\\site-packages (from smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: botocore<1.13.0,>=1.12.7 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: s3transfer<0.2.0,>=0.1.10 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: docutils>=0.10 in c:\\users\\monis\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.7->boto3->smart-open>=1.2.1->gensim)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\users\\monis\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.7->boto3->smart-open>=1.2.1->gensim)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.1, however version 19.0 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\monis\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\monis\\Anaconda3\\lib\\runpy.py:125: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn torch tqdm nltk lazyme requests gensim\n",
    "!python -m nltk.downloader movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "from tsundoku.word2vec_hints import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "\n",
    "- <a href=\"#section-3-0\">**3.0. Data Preparation**</a>\n",
    "  - <a href=\"#section-3-0-1\">3.0.1. *Vocabulary*</a>\n",
    "    - <a href=\"#section-3-0-1-a\"> Pet Peeve: using `gensim`</a>\n",
    "  - <a href=\"#section-3-0-2\">3.0.2. *Dataset*</a>  (<a href=\"#section-3-0-2-hints\">Hints</a>)\n",
    "    - <a href=\"#section-3-0-2-return-dict\">Return `dict` in `__getitem__()`</a>\n",
    "    - <a href=\"#section-3-0-2-labeleddata\">Try `LabeledDataset`</a>\n",
    "<br><br>\n",
    "- <a href=\"#section-3-1\">**3.1. Word2Vec from Scratch**</a>\n",
    "  - <a href=\"#section-3-1-1\">3.1.1. *CBOW*</a>\n",
    "  - <a href=\"#section-3-1-2\">3.1.2. *Skipgram*</a>\n",
    "  - <a href=\"#section-3-1-3\">3.1.3. *Word2Vec Dataset*</a> (<a href=\"#section-3-1-3-hint\">Hints</a>)\n",
    "  - <a href=\"#section-3-1-4-hint\">3.1.4. *Train a CBOW model*</a>\n",
    "    - <a href=\"#section-3-1-4-fill-cbow\">Fill in the CBOW model</a>\n",
    "    - <a href=\"#section-3-1-4-train-cbow\">Train the model (*for real*)</a>\n",
    "    - <a href=\"#section-3-1-4-evaluate-cbow\">Evaluate the model</a>\n",
    "    - <a href=\"#section-3-1-4-load-model\">Load model at specific epoch</a>\n",
    "  - <a href=\"#section-3-1-5\">3.1.5. *Train a Skipgram model*</a>\n",
    "    - <a href=\"#section-3-1-5-forward\">Take a closer look at `forward()`</a>\n",
    "    - <a href=\"#section-3-1-5-train\">Train the model (*for real*)</a>\n",
    "    - <a href=\"section-3-1-5-evaluate\">Evaluate the model</a>\n",
    "  - <a href=\"#section-3-1-6\">3.1.6. *Loading Pre-trained Embeddings*</a>\n",
    "    - <a href=\"#section-3-1-6-vocab\">Override the Embedding vocabulary</a>\n",
    "    - <a href=\"#section-3-1-6-pretrained\">Override the Embedding weights</a>\n",
    "    - <a href=\"#section-3-1-6-eval-skipgram\">Evaluate on the Skipgram task</a>\n",
    "    - <a href=\"#section-3-1-6-eval-cbow\">Evaluate on the CBOW task</a>\n",
    "    - <a href=\"#section-3-1-6-unfreeze-finetune\">Unfreeeze and finetune</a>\n",
    "    - <a href=\"#section-3-1-6-reval-cbow\">Re-evaluate on the CBOW task</a>\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0\"></a>\n",
    "# 3.0. Data Preparation\n",
    "\n",
    "Before we train our own embeddings, lets first understand how to read text data into pytorch.\n",
    "The native pytorch way to load datasets is to use the `torch.utils.data.Dataset` object.\n",
    "\n",
    "There are already several other libraries that help with loading text datasets, e.g. \n",
    "\n",
    " - FastAI https://docs.fast.ai/text.data.html\n",
    " - AllenNLP https://allenai.github.io/allennlp-docs/api/allennlp.data.dataset.html\n",
    " - Torch Text https://github.com/pytorch/text#data\n",
    " - Texar https://texar.readthedocs.io/en/latest/code/data.html#id4 \n",
    " - SpaCy https://github.com/explosion/thinc\n",
    " \n",
    "\n",
    "But to truly understand and use it for the custom datasets you'll see at work, lets learn it the native way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-1\"></a>\n",
    "## 3.0.1  Vocabulary\n",
    "\n",
    "Given a text, the first thing to do is to build a vocabulary (i.e. a dictionary of unique words) and assign an index to each unique word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "from functools import partial\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # Use the default NLTK tokenizer.\n",
    "    from nltk import word_tokenize, sent_tokenize \n",
    "    # Testing whether it works. \n",
    "    # Sometimes it doesn't work on some machines because of setup issues.\n",
    "    word_tokenize(sent_tokenize(\"This is a foobar sentence. Yes it is.\")[0])\n",
    "except: # Use a naive sentence tokenizer and toktok.\n",
    "    import re\n",
    "    from nltk.tokenize import ToktokTokenizer\n",
    "    # See https://stackoverflow.com/a/25736515/610569\n",
    "    sent_tokenize = lambda x: re.split(r'(?<=[^A-Z].[.?]) +(?=[A-Z])', x)\n",
    "    # Use the toktok tokenizer that requires no dependencies.\n",
    "    toktok = ToktokTokenizer()\n",
    "    word_tokenize = word_tokenize = toktok.tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text = \"\"\"Language users never choose words randomly, and language is essentially\n",
    "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
    "posits randomness. Hence, when we look at linguistic phenomena in corpora, \n",
    "the null hypothesis will never be true. Moreover, where there is enough\n",
    "data, we shall (almost) always be able to establish that it is not true. In\n",
    "corpus studies, we frequently do have enough data, so the fact that a relation \n",
    "between two phenomena is demonstrably non-random, does not support the inference \n",
    "that it is not arbitrary. We present experimental evidence\n",
    "of how arbitrary associations between word frequencies and corpora are\n",
    "systematically non-random. We review literature in which hypothesis testing \n",
    "has been used, and show how it has often led to unhelpful or misleading results.\"\"\".lower()\n",
    "\n",
    "tokenized_text = [word_tokenize(sent) for sent in sent_tokenize(text)]\n",
    "\n",
    "uniq_tokens = set(chain(*tokenized_text))\n",
    "\n",
    "vocab = {}   # Assign indices to every word.\n",
    "idx2tok = {} # Also keep an dict of index to words.\n",
    "for i, token in enumerate(uniq_tokens):\n",
    "    vocab[token] = i\n",
    "    idx2tok[i] = token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(': 2,\n",
       " ')': 14,\n",
       " ',': 17,\n",
       " '.': 20,\n",
       " 'a': 10,\n",
       " 'able': 29,\n",
       " 'almost': 64,\n",
       " 'always': 80,\n",
       " 'and': 16,\n",
       " 'arbitrary': 48,\n",
       " 'are': 30,\n",
       " 'associations': 63,\n",
       " 'at': 12,\n",
       " 'be': 50,\n",
       " 'been': 73,\n",
       " 'between': 82,\n",
       " 'choose': 69,\n",
       " 'corpora': 11,\n",
       " 'corpus': 36,\n",
       " 'data': 74,\n",
       " 'demonstrably': 84,\n",
       " 'do': 39,\n",
       " 'does': 47,\n",
       " 'enough': 76,\n",
       " 'essentially': 85,\n",
       " 'establish': 41,\n",
       " 'evidence': 8,\n",
       " 'experimental': 72,\n",
       " 'fact': 18,\n",
       " 'frequencies': 57,\n",
       " 'frequently': 28,\n",
       " 'has': 38,\n",
       " 'have': 21,\n",
       " 'hence': 34,\n",
       " 'how': 31,\n",
       " 'hypothesis': 32,\n",
       " 'in': 19,\n",
       " 'inference': 59,\n",
       " 'is': 83,\n",
       " 'it': 65,\n",
       " 'language': 45,\n",
       " 'led': 62,\n",
       " 'linguistic': 53,\n",
       " 'literature': 67,\n",
       " 'look': 86,\n",
       " 'misleading': 26,\n",
       " 'moreover': 55,\n",
       " 'never': 78,\n",
       " 'non-random': 22,\n",
       " 'not': 51,\n",
       " 'null': 0,\n",
       " 'of': 42,\n",
       " 'often': 58,\n",
       " 'or': 33,\n",
       " 'phenomena': 75,\n",
       " 'posits': 9,\n",
       " 'present': 66,\n",
       " 'randomly': 27,\n",
       " 'randomness': 54,\n",
       " 'relation': 13,\n",
       " 'results': 44,\n",
       " 'review': 7,\n",
       " 'shall': 37,\n",
       " 'show': 43,\n",
       " 'so': 56,\n",
       " 'statistical': 3,\n",
       " 'studies': 6,\n",
       " 'support': 4,\n",
       " 'systematically': 49,\n",
       " 'testing': 23,\n",
       " 'that': 40,\n",
       " 'the': 35,\n",
       " 'there': 46,\n",
       " 'to': 1,\n",
       " 'true': 25,\n",
       " 'two': 60,\n",
       " 'unhelpful': 24,\n",
       " 'used': 15,\n",
       " 'users': 81,\n",
       " 'uses': 68,\n",
       " 'we': 61,\n",
       " 'when': 71,\n",
       " 'where': 77,\n",
       " 'which': 5,\n",
       " 'will': 52,\n",
       " 'word': 79,\n",
       " 'words': 70}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve the index of the word 'corpora'\n",
    "vocab['non-random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[45, 81, 78, 69, 70, 27, 17, 16, 45, 83, 85, 22, 20]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The indexed representation of the first sentence.\n",
    "\n",
    "sent0 = tokenized_text[0]\n",
    "\n",
    "[vocab[token] for token in sent0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-1-a\"></a>\n",
    "\n",
    "### Pet Peeve (Gensim)\n",
    "\n",
    "I (Liling) don't really like to write my own vectorizer the `gensim` has functions that are optimized for such operations. In fact, I've written a [whole preprocessing pipeline library for me to use for language modelling and machine translation purposes](https://github.com/alvations/komorebi/blob/master/komorebi/text.py) =)\n",
    "\n",
    "Using `gensim`, I would have written the above as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "vocab = Dictionary(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: ',',\n",
       " 1: '.',\n",
       " 2: 'and',\n",
       " 3: 'choose',\n",
       " 4: 'essentially',\n",
       " 5: 'is',\n",
       " 6: 'language',\n",
       " 7: 'never',\n",
       " 8: 'non-random',\n",
       " 9: 'randomly',\n",
       " 10: 'users',\n",
       " 11: 'words',\n",
       " 12: 'a',\n",
       " 13: 'hypothesis',\n",
       " 14: 'null',\n",
       " 15: 'posits',\n",
       " 16: 'randomness',\n",
       " 17: 'statistical',\n",
       " 18: 'testing',\n",
       " 19: 'uses',\n",
       " 20: 'which',\n",
       " 21: 'at',\n",
       " 22: 'be',\n",
       " 23: 'corpora',\n",
       " 24: 'hence',\n",
       " 25: 'in',\n",
       " 26: 'linguistic',\n",
       " 27: 'look',\n",
       " 28: 'phenomena',\n",
       " 29: 'the',\n",
       " 30: 'true',\n",
       " 31: 'we',\n",
       " 32: 'when',\n",
       " 33: 'will',\n",
       " 34: '(',\n",
       " 35: ')',\n",
       " 36: 'able',\n",
       " 37: 'almost',\n",
       " 38: 'always',\n",
       " 39: 'data',\n",
       " 40: 'enough',\n",
       " 41: 'establish',\n",
       " 42: 'it',\n",
       " 43: 'moreover',\n",
       " 44: 'not',\n",
       " 45: 'shall',\n",
       " 46: 'that',\n",
       " 47: 'there',\n",
       " 48: 'to',\n",
       " 49: 'where',\n",
       " 50: 'arbitrary',\n",
       " 51: 'between',\n",
       " 52: 'corpus',\n",
       " 53: 'demonstrably',\n",
       " 54: 'do',\n",
       " 55: 'does',\n",
       " 56: 'fact',\n",
       " 57: 'frequently',\n",
       " 58: 'have',\n",
       " 59: 'inference',\n",
       " 60: 'relation',\n",
       " 61: 'so',\n",
       " 62: 'studies',\n",
       " 63: 'support',\n",
       " 64: 'two',\n",
       " 65: 'are',\n",
       " 66: 'associations',\n",
       " 67: 'evidence',\n",
       " 68: 'experimental',\n",
       " 69: 'frequencies',\n",
       " 70: 'how',\n",
       " 71: 'of',\n",
       " 72: 'present',\n",
       " 73: 'systematically',\n",
       " 74: 'word',\n",
       " 75: 'been',\n",
       " 76: 'has',\n",
       " 77: 'led',\n",
       " 78: 'literature',\n",
       " 79: 'misleading',\n",
       " 80: 'often',\n",
       " 81: 'or',\n",
       " 82: 'results',\n",
       " 83: 'review',\n",
       " 84: 'show',\n",
       " 85: 'unhelpful',\n",
       " 86: 'used'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the key-value order is different of gensim from the native Python's\n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.token2id['corpora']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 10, 7, 3, 11, 9, 0, 2, 6, 5, 4, 8, 1]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.doc2idx(sent0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"indexed form\" of the tokens in the sentence forms the ***vectorized*** input to the `nn.Embedding` layer in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-2\"></a>\n",
    "\n",
    "# 3.0.2 Dataset\n",
    "\n",
    "Lets try creating a `torch.utils.data.Dataset` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Text(Dataset):\n",
    "    def __init__(self, tokenized_texts):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self.vocab = Dictionary(tokenized_texts)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        # Hint: You want to return a vectorized sentence here.\n",
    "        return {'x': self.vectorize(self.sents[index])}\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-2-hints\"></a>\n",
    "## Hints to the above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: To see the hint and partial code for the cell above, uncomment the following line.\n",
    "##hint_dataset_vectorize()\n",
    "##code_text_dataset_vectorize()\n",
    "\n",
    "# Option 2: \"I give up just, run the code for me\" \n",
    "# Uncomment the next two lines, if you really gave up... \n",
    "#full_code_text_dataset_vectorize()\n",
    "##from tsundoku.word2vec import Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'present',\n",
       " 'experimental',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'how',\n",
       " 'arbitrary',\n",
       " 'associations',\n",
       " 'between',\n",
       " 'word',\n",
       " 'frequencies',\n",
       " 'and',\n",
       " 'corpora',\n",
       " 'are',\n",
       " 'systematically',\n",
       " 'non-random',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_dataset = Text(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': [31, 72, 68, 67, 71, 70, 50, 66, 51, 74, 69, 2, 23, 65, 73, 8, 1]}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_dataset[5] # First sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-2-return-dict\"></a>\n",
    "\n",
    "### Return `dict` in `__getitem__()`\n",
    "\n",
    "This is nice if we're just representing sentences/documents by their indices but when we're doing machine learning, we usually have `X` and `Y`. \n",
    "\n",
    "If we have labels for the each sentence, we can also put it into to `__getitem__()` by having it return a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class LabeledText(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self.labels = labels # Sentence level labels.\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        return {'X': self.vectorize(self.sents[index]), 'Y': self.labels[index]}\n",
    "\n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-0-2-labeleddata\"></a>\n",
    "\n",
    "### Lets try the `LabeledDataset` on a movie review corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 2000/2000 [00:18<00:00, 108.12it/s]\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "labels = []\n",
    "\n",
    "for fileid in tqdm(movie_reviews.fileids()):\n",
    "    label = fileid.split('/')[0]\n",
    "    doc = word_tokenize(movie_reviews.open(fileid).read())\n",
    "    documents.append(doc)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['plot',\n",
       " ':',\n",
       " 'two',\n",
       " 'teen',\n",
       " 'couples',\n",
       " 'go',\n",
       " 'to',\n",
       " 'a',\n",
       " 'church',\n",
       " 'party',\n",
       " ',',\n",
       " 'drink',\n",
       " 'and',\n",
       " 'then',\n",
       " 'drive',\n",
       " '.',\n",
       " 'they',\n",
       " 'get',\n",
       " 'into',\n",
       " 'an',\n",
       " 'accident',\n",
       " '.',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'guys',\n",
       " 'dies',\n",
       " ',',\n",
       " 'but',\n",
       " 'his',\n",
       " 'girlfriend',\n",
       " 'continues',\n",
       " 'to',\n",
       " 'see',\n",
       " 'him',\n",
       " 'in',\n",
       " 'her',\n",
       " 'life',\n",
       " ',',\n",
       " 'and',\n",
       " 'has',\n",
       " 'nightmares',\n",
       " '.',\n",
       " 'what',\n",
       " \"'s\",\n",
       " 'the',\n",
       " 'deal',\n",
       " '?',\n",
       " 'watch',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'and',\n",
       " '``',\n",
       " 'sorta',\n",
       " '``',\n",
       " 'find',\n",
       " 'out',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'critique',\n",
       " ':',\n",
       " 'a',\n",
       " 'mind-fuck',\n",
       " 'movie',\n",
       " 'for',\n",
       " 'the',\n",
       " 'teen',\n",
       " 'generation',\n",
       " 'that',\n",
       " 'touches',\n",
       " 'on',\n",
       " 'a',\n",
       " 'very',\n",
       " 'cool',\n",
       " 'idea',\n",
       " ',',\n",
       " 'but',\n",
       " 'presents',\n",
       " 'it',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'bad',\n",
       " 'package',\n",
       " '.',\n",
       " 'which',\n",
       " 'is',\n",
       " 'what',\n",
       " 'makes',\n",
       " 'this',\n",
       " 'review',\n",
       " 'an',\n",
       " 'even',\n",
       " 'harder',\n",
       " 'one',\n",
       " 'to',\n",
       " 'write',\n",
       " ',',\n",
       " 'since',\n",
       " 'i',\n",
       " 'generally',\n",
       " 'applaud',\n",
       " 'films',\n",
       " 'which',\n",
       " 'attempt',\n",
       " 'to',\n",
       " 'break',\n",
       " 'the',\n",
       " 'mold',\n",
       " ',',\n",
       " 'mess',\n",
       " 'with',\n",
       " 'your',\n",
       " 'head',\n",
       " 'and',\n",
       " 'such',\n",
       " '(',\n",
       " 'lost',\n",
       " 'highway',\n",
       " '&',\n",
       " 'memento',\n",
       " ')',\n",
       " ',',\n",
       " 'but',\n",
       " 'there',\n",
       " 'are',\n",
       " 'good',\n",
       " 'and',\n",
       " 'bad',\n",
       " 'ways',\n",
       " 'of',\n",
       " 'making',\n",
       " 'all',\n",
       " 'types',\n",
       " 'of',\n",
       " 'films',\n",
       " ',',\n",
       " 'and',\n",
       " 'these',\n",
       " 'folks',\n",
       " 'just',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'snag',\n",
       " 'this',\n",
       " 'one',\n",
       " 'correctly',\n",
       " '.',\n",
       " 'they',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'taken',\n",
       " 'this',\n",
       " 'pretty',\n",
       " 'neat',\n",
       " 'concept',\n",
       " ',',\n",
       " 'but',\n",
       " 'executed',\n",
       " 'it',\n",
       " 'terribly',\n",
       " '.',\n",
       " 'so',\n",
       " 'what',\n",
       " 'are',\n",
       " 'the',\n",
       " 'problems',\n",
       " 'with',\n",
       " 'the',\n",
       " 'movie',\n",
       " '?',\n",
       " 'well',\n",
       " ',',\n",
       " 'its',\n",
       " 'main',\n",
       " 'problem',\n",
       " 'is',\n",
       " 'that',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'simply',\n",
       " 'too',\n",
       " 'jumbled',\n",
       " '.',\n",
       " 'it',\n",
       " 'starts',\n",
       " 'off',\n",
       " '``',\n",
       " 'normal',\n",
       " '``',\n",
       " 'but',\n",
       " 'then',\n",
       " 'downshifts',\n",
       " 'into',\n",
       " 'this',\n",
       " '``',\n",
       " 'fantasy',\n",
       " '``',\n",
       " 'world',\n",
       " 'in',\n",
       " 'which',\n",
       " 'you',\n",
       " ',',\n",
       " 'as',\n",
       " 'an',\n",
       " 'audience',\n",
       " 'member',\n",
       " ',',\n",
       " 'have',\n",
       " 'no',\n",
       " 'idea',\n",
       " 'what',\n",
       " \"'s\",\n",
       " 'going',\n",
       " 'on',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'dreams',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'characters',\n",
       " 'coming',\n",
       " 'back',\n",
       " 'from',\n",
       " 'the',\n",
       " 'dead',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'others',\n",
       " 'who',\n",
       " 'look',\n",
       " 'like',\n",
       " 'the',\n",
       " 'dead',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'strange',\n",
       " 'apparitions',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'disappearances',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'a',\n",
       " 'looooot',\n",
       " 'of',\n",
       " 'chase',\n",
       " 'scenes',\n",
       " ',',\n",
       " 'there',\n",
       " 'are',\n",
       " 'tons',\n",
       " 'of',\n",
       " 'weird',\n",
       " 'things',\n",
       " 'that',\n",
       " 'happen',\n",
       " ',',\n",
       " 'and',\n",
       " 'most',\n",
       " 'of',\n",
       " 'it',\n",
       " 'is',\n",
       " 'simply',\n",
       " 'not',\n",
       " 'explained',\n",
       " '.',\n",
       " 'now',\n",
       " 'i',\n",
       " 'personally',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'mind',\n",
       " 'trying',\n",
       " 'to',\n",
       " 'unravel',\n",
       " 'a',\n",
       " 'film',\n",
       " 'every',\n",
       " 'now',\n",
       " 'and',\n",
       " 'then',\n",
       " ',',\n",
       " 'but',\n",
       " 'when',\n",
       " 'all',\n",
       " 'it',\n",
       " 'does',\n",
       " 'is',\n",
       " 'give',\n",
       " 'me',\n",
       " 'the',\n",
       " 'same',\n",
       " 'clue',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'again',\n",
       " ',',\n",
       " 'i',\n",
       " 'get',\n",
       " 'kind',\n",
       " 'of',\n",
       " 'fed',\n",
       " 'up',\n",
       " 'after',\n",
       " 'a',\n",
       " 'while',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'this',\n",
       " 'film',\n",
       " \"'s\",\n",
       " 'biggest',\n",
       " 'problem',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'obviously',\n",
       " 'got',\n",
       " 'this',\n",
       " 'big',\n",
       " 'secret',\n",
       " 'to',\n",
       " 'hide',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'want',\n",
       " 'to',\n",
       " 'hide',\n",
       " 'it',\n",
       " 'completely',\n",
       " 'until',\n",
       " 'its',\n",
       " 'final',\n",
       " 'five',\n",
       " 'minutes',\n",
       " '.',\n",
       " 'and',\n",
       " 'do',\n",
       " 'they',\n",
       " 'make',\n",
       " 'things',\n",
       " 'entertaining',\n",
       " ',',\n",
       " 'thrilling',\n",
       " 'or',\n",
       " 'even',\n",
       " 'engaging',\n",
       " ',',\n",
       " 'in',\n",
       " 'the',\n",
       " 'meantime',\n",
       " '?',\n",
       " 'not',\n",
       " 'really',\n",
       " '.',\n",
       " 'the',\n",
       " 'sad',\n",
       " 'part',\n",
       " 'is',\n",
       " 'that',\n",
       " 'the',\n",
       " 'arrow',\n",
       " 'and',\n",
       " 'i',\n",
       " 'both',\n",
       " 'dig',\n",
       " 'on',\n",
       " 'flicks',\n",
       " 'like',\n",
       " 'this',\n",
       " ',',\n",
       " 'so',\n",
       " 'we',\n",
       " 'actually',\n",
       " 'figured',\n",
       " 'most',\n",
       " 'of',\n",
       " 'it',\n",
       " 'out',\n",
       " 'by',\n",
       " 'the',\n",
       " 'half-way',\n",
       " 'point',\n",
       " ',',\n",
       " 'so',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strangeness',\n",
       " 'after',\n",
       " 'that',\n",
       " 'did',\n",
       " 'start',\n",
       " 'to',\n",
       " 'make',\n",
       " 'a',\n",
       " 'little',\n",
       " 'bit',\n",
       " 'of',\n",
       " 'sense',\n",
       " ',',\n",
       " 'but',\n",
       " 'it',\n",
       " 'still',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'the',\n",
       " 'make',\n",
       " 'the',\n",
       " 'film',\n",
       " 'all',\n",
       " 'that',\n",
       " 'more',\n",
       " 'entertaining',\n",
       " '.',\n",
       " 'i',\n",
       " 'guess',\n",
       " 'the',\n",
       " 'bottom',\n",
       " 'line',\n",
       " 'with',\n",
       " 'movies',\n",
       " 'like',\n",
       " 'this',\n",
       " 'is',\n",
       " 'that',\n",
       " 'you',\n",
       " 'should',\n",
       " 'always',\n",
       " 'make',\n",
       " 'sure',\n",
       " 'that',\n",
       " 'the',\n",
       " 'audience',\n",
       " 'is',\n",
       " '``',\n",
       " 'into',\n",
       " 'it',\n",
       " '``',\n",
       " 'even',\n",
       " 'before',\n",
       " 'they',\n",
       " 'are',\n",
       " 'given',\n",
       " 'the',\n",
       " 'secret',\n",
       " 'password',\n",
       " 'to',\n",
       " 'enter',\n",
       " 'your',\n",
       " 'world',\n",
       " 'of',\n",
       " 'understanding',\n",
       " '.',\n",
       " 'i',\n",
       " 'mean',\n",
       " ',',\n",
       " 'showing',\n",
       " 'melissa',\n",
       " 'sagemiller',\n",
       " 'running',\n",
       " 'away',\n",
       " 'from',\n",
       " 'visions',\n",
       " 'for',\n",
       " 'about',\n",
       " '20',\n",
       " 'minutes',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'movie',\n",
       " 'is',\n",
       " 'just',\n",
       " 'plain',\n",
       " 'lazy',\n",
       " '!',\n",
       " '!',\n",
       " 'okay',\n",
       " ',',\n",
       " 'we',\n",
       " 'get',\n",
       " 'it',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'there',\n",
       " 'are',\n",
       " 'people',\n",
       " 'chasing',\n",
       " 'her',\n",
       " 'and',\n",
       " 'we',\n",
       " 'do',\n",
       " \"n't\",\n",
       " 'know',\n",
       " 'who',\n",
       " 'they',\n",
       " 'are',\n",
       " '.',\n",
       " 'do',\n",
       " 'we',\n",
       " 'really',\n",
       " 'need',\n",
       " 'to',\n",
       " 'see',\n",
       " 'it',\n",
       " 'over',\n",
       " 'and',\n",
       " 'over',\n",
       " 'again',\n",
       " '?',\n",
       " 'how',\n",
       " 'about',\n",
       " 'giving',\n",
       " 'us',\n",
       " 'different',\n",
       " 'scenes',\n",
       " 'offering',\n",
       " 'further',\n",
       " 'insight',\n",
       " 'into',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'strangeness',\n",
       " 'going',\n",
       " 'down',\n",
       " 'in',\n",
       " 'the',\n",
       " 'movie',\n",
       " '?',\n",
       " 'apparently',\n",
       " ',',\n",
       " 'the',\n",
       " 'studio',\n",
       " 'took',\n",
       " 'this',\n",
       " 'film',\n",
       " 'away',\n",
       " 'from',\n",
       " 'its',\n",
       " 'director',\n",
       " 'and',\n",
       " 'chopped',\n",
       " 'it',\n",
       " 'up',\n",
       " 'themselves',\n",
       " ',',\n",
       " 'and',\n",
       " 'it',\n",
       " 'shows',\n",
       " '.',\n",
       " 'there',\n",
       " 'might',\n",
       " \"'ve\",\n",
       " 'been',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'decent',\n",
       " 'teen',\n",
       " 'mind-fuck',\n",
       " 'movie',\n",
       " 'in',\n",
       " 'here',\n",
       " 'somewhere',\n",
       " ',',\n",
       " 'but',\n",
       " 'i',\n",
       " 'guess',\n",
       " '``',\n",
       " 'the',\n",
       " 'suits',\n",
       " '``',\n",
       " 'decided',\n",
       " 'that',\n",
       " 'turning',\n",
       " 'it',\n",
       " 'into',\n",
       " 'a',\n",
       " 'music',\n",
       " 'video',\n",
       " 'with',\n",
       " 'little',\n",
       " 'edge',\n",
       " ',',\n",
       " 'would',\n",
       " 'make',\n",
       " 'more',\n",
       " 'sense',\n",
       " '.',\n",
       " 'the',\n",
       " 'actors',\n",
       " 'are',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'for',\n",
       " 'the',\n",
       " 'most',\n",
       " 'part',\n",
       " ',',\n",
       " 'although',\n",
       " 'wes',\n",
       " 'bentley',\n",
       " 'just',\n",
       " 'seemed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'playing',\n",
       " 'the',\n",
       " 'exact',\n",
       " 'same',\n",
       " 'character',\n",
       " 'that',\n",
       " 'he',\n",
       " 'did',\n",
       " 'in',\n",
       " 'american',\n",
       " 'beauty',\n",
       " ',',\n",
       " 'only',\n",
       " 'in',\n",
       " 'a',\n",
       " 'new',\n",
       " 'neighborhood',\n",
       " '.',\n",
       " 'but',\n",
       " 'my',\n",
       " 'biggest',\n",
       " 'kudos',\n",
       " 'go',\n",
       " 'out',\n",
       " 'to',\n",
       " 'sagemiller',\n",
       " ',',\n",
       " 'who',\n",
       " 'holds',\n",
       " 'her',\n",
       " 'own',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'entire',\n",
       " 'film',\n",
       " ',',\n",
       " 'and',\n",
       " 'actually',\n",
       " 'has',\n",
       " 'you',\n",
       " 'feeling',\n",
       " 'her',\n",
       " 'character',\n",
       " \"'s\",\n",
       " 'unraveling',\n",
       " '.',\n",
       " 'overall',\n",
       " ',',\n",
       " 'the',\n",
       " 'film',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'stick',\n",
       " 'because',\n",
       " 'it',\n",
       " 'does',\n",
       " \"n't\",\n",
       " 'entertain',\n",
       " ',',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'confusing',\n",
       " ',',\n",
       " 'it',\n",
       " 'rarely',\n",
       " 'excites',\n",
       " 'and',\n",
       " 'it',\n",
       " 'feels',\n",
       " 'pretty',\n",
       " 'redundant',\n",
       " 'for',\n",
       " 'most',\n",
       " 'of',\n",
       " 'its',\n",
       " 'runtime',\n",
       " ',',\n",
       " 'despite',\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'cool',\n",
       " 'ending',\n",
       " 'and',\n",
       " 'explanation',\n",
       " 'to',\n",
       " 'all',\n",
       " 'of',\n",
       " 'the',\n",
       " 'craziness',\n",
       " 'that',\n",
       " 'came',\n",
       " 'before',\n",
       " 'it',\n",
       " '.',\n",
       " 'oh',\n",
       " ',',\n",
       " 'and',\n",
       " 'by',\n",
       " 'the',\n",
       " 'way',\n",
       " ',',\n",
       " 'this',\n",
       " 'is',\n",
       " 'not',\n",
       " 'a',\n",
       " 'horror',\n",
       " 'or',\n",
       " 'teen',\n",
       " 'slasher',\n",
       " 'flick',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'just',\n",
       " 'packaged',\n",
       " 'to',\n",
       " 'look',\n",
       " 'that',\n",
       " 'way',\n",
       " 'because',\n",
       " 'someone',\n",
       " 'is',\n",
       " 'apparently',\n",
       " 'assuming',\n",
       " 'that',\n",
       " 'the',\n",
       " 'genre',\n",
       " 'is',\n",
       " 'still',\n",
       " 'hot',\n",
       " 'with',\n",
       " 'the',\n",
       " 'kids',\n",
       " '.',\n",
       " 'it',\n",
       " 'also',\n",
       " 'wrapped',\n",
       " 'production',\n",
       " 'two',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'and',\n",
       " 'has',\n",
       " 'been',\n",
       " 'sitting',\n",
       " 'on',\n",
       " 'the',\n",
       " 'shelves',\n",
       " 'ever',\n",
       " 'since',\n",
       " '.',\n",
       " 'whatever',\n",
       " '.',\n",
       " '.',\n",
       " '.',\n",
       " 'skip',\n",
       " 'it',\n",
       " '!',\n",
       " 'where',\n",
       " \"'s\",\n",
       " 'joblo',\n",
       " 'coming',\n",
       " 'from',\n",
       " '?',\n",
       " 'a',\n",
       " 'nightmare',\n",
       " 'of',\n",
       " 'elm',\n",
       " 'street',\n",
       " '3',\n",
       " '(',\n",
       " '7/10',\n",
       " ')',\n",
       " '-',\n",
       " 'blair',\n",
       " 'witch',\n",
       " '2',\n",
       " '(',\n",
       " '7/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'crow',\n",
       " '(',\n",
       " '9/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'crow',\n",
       " ':',\n",
       " 'salvation',\n",
       " '(',\n",
       " '4/10',\n",
       " ')',\n",
       " '-',\n",
       " 'lost',\n",
       " 'highway',\n",
       " '(',\n",
       " '10/10',\n",
       " ')',\n",
       " '-',\n",
       " 'memento',\n",
       " '(',\n",
       " '10/10',\n",
       " ')',\n",
       " '-',\n",
       " 'the',\n",
       " 'others',\n",
       " '(',\n",
       " '9/10',\n",
       " ')',\n",
       " '-',\n",
       " 'stir',\n",
       " 'of',\n",
       " 'echoes',\n",
       " '(',\n",
       " '8/10',\n",
       " ')']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_dataset = LabeledText(documents, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': [243,\n",
       "  17,\n",
       "  314,\n",
       "  294,\n",
       "  77,\n",
       "  140,\n",
       "  307,\n",
       "  20,\n",
       "  68,\n",
       "  237,\n",
       "  6,\n",
       "  97,\n",
       "  34,\n",
       "  299,\n",
       "  98,\n",
       "  8,\n",
       "  302,\n",
       "  135,\n",
       "  167,\n",
       "  33,\n",
       "  22,\n",
       "  8,\n",
       "  226,\n",
       "  220,\n",
       "  297,\n",
       "  145,\n",
       "  87,\n",
       "  6,\n",
       "  60,\n",
       "  158,\n",
       "  136,\n",
       "  74,\n",
       "  307,\n",
       "  262,\n",
       "  157,\n",
       "  165,\n",
       "  153,\n",
       "  179,\n",
       "  6,\n",
       "  34,\n",
       "  149,\n",
       "  214,\n",
       "  8,\n",
       "  333,\n",
       "  2,\n",
       "  297,\n",
       "  82,\n",
       "  18,\n",
       "  326,\n",
       "  297,\n",
       "  204,\n",
       "  34,\n",
       "  19,\n",
       "  280,\n",
       "  19,\n",
       "  124,\n",
       "  230,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  79,\n",
       "  17,\n",
       "  20,\n",
       "  199,\n",
       "  204,\n",
       "  129,\n",
       "  297,\n",
       "  294,\n",
       "  133,\n",
       "  296,\n",
       "  311,\n",
       "  225,\n",
       "  20,\n",
       "  322,\n",
       "  75,\n",
       "  164,\n",
       "  6,\n",
       "  60,\n",
       "  245,\n",
       "  169,\n",
       "  165,\n",
       "  20,\n",
       "  322,\n",
       "  46,\n",
       "  234,\n",
       "  8,\n",
       "  337,\n",
       "  168,\n",
       "  333,\n",
       "  188,\n",
       "  304,\n",
       "  253,\n",
       "  33,\n",
       "  108,\n",
       "  148,\n",
       "  226,\n",
       "  307,\n",
       "  345,\n",
       "  6,\n",
       "  272,\n",
       "  163,\n",
       "  132,\n",
       "  37,\n",
       "  122,\n",
       "  337,\n",
       "  42,\n",
       "  307,\n",
       "  59,\n",
       "  297,\n",
       "  201,\n",
       "  6,\n",
       "  196,\n",
       "  341,\n",
       "  348,\n",
       "  152,\n",
       "  34,\n",
       "  290,\n",
       "  4,\n",
       "  185,\n",
       "  156,\n",
       "  1,\n",
       "  195,\n",
       "  5,\n",
       "  6,\n",
       "  60,\n",
       "  300,\n",
       "  38,\n",
       "  142,\n",
       "  34,\n",
       "  46,\n",
       "  328,\n",
       "  220,\n",
       "  189,\n",
       "  28,\n",
       "  315,\n",
       "  220,\n",
       "  122,\n",
       "  6,\n",
       "  34,\n",
       "  301,\n",
       "  128,\n",
       "  173,\n",
       "  86,\n",
       "  208,\n",
       "  276,\n",
       "  304,\n",
       "  226,\n",
       "  76,\n",
       "  8,\n",
       "  302,\n",
       "  263,\n",
       "  307,\n",
       "  150,\n",
       "  293,\n",
       "  304,\n",
       "  246,\n",
       "  209,\n",
       "  72,\n",
       "  6,\n",
       "  60,\n",
       "  113,\n",
       "  169,\n",
       "  295,\n",
       "  8,\n",
       "  277,\n",
       "  333,\n",
       "  38,\n",
       "  297,\n",
       "  248,\n",
       "  341,\n",
       "  297,\n",
       "  204,\n",
       "  18,\n",
       "  331,\n",
       "  6,\n",
       "  170,\n",
       "  186,\n",
       "  247,\n",
       "  168,\n",
       "  296,\n",
       "  169,\n",
       "  2,\n",
       "  271,\n",
       "  309,\n",
       "  172,\n",
       "  8,\n",
       "  169,\n",
       "  282,\n",
       "  221,\n",
       "  19,\n",
       "  216,\n",
       "  19,\n",
       "  60,\n",
       "  299,\n",
       "  95,\n",
       "  167,\n",
       "  304,\n",
       "  19,\n",
       "  116,\n",
       "  19,\n",
       "  342,\n",
       "  165,\n",
       "  337,\n",
       "  347,\n",
       "  6,\n",
       "  40,\n",
       "  33,\n",
       "  43,\n",
       "  194,\n",
       "  6,\n",
       "  150,\n",
       "  215,\n",
       "  164,\n",
       "  333,\n",
       "  2,\n",
       "  141,\n",
       "  225,\n",
       "  8,\n",
       "  300,\n",
       "  38,\n",
       "  96,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  64,\n",
       "  70,\n",
       "  45,\n",
       "  130,\n",
       "  297,\n",
       "  81,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  229,\n",
       "  339,\n",
       "  183,\n",
       "  180,\n",
       "  297,\n",
       "  81,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  286,\n",
       "  36,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  91,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  20,\n",
       "  184,\n",
       "  220,\n",
       "  65,\n",
       "  260,\n",
       "  6,\n",
       "  300,\n",
       "  38,\n",
       "  308,\n",
       "  220,\n",
       "  330,\n",
       "  303,\n",
       "  296,\n",
       "  147,\n",
       "  6,\n",
       "  34,\n",
       "  203,\n",
       "  220,\n",
       "  169,\n",
       "  168,\n",
       "  271,\n",
       "  217,\n",
       "  114,\n",
       "  8,\n",
       "  218,\n",
       "  163,\n",
       "  240,\n",
       "  92,\n",
       "  208,\n",
       "  198,\n",
       "  312,\n",
       "  307,\n",
       "  317,\n",
       "  20,\n",
       "  121,\n",
       "  110,\n",
       "  218,\n",
       "  34,\n",
       "  299,\n",
       "  6,\n",
       "  60,\n",
       "  335,\n",
       "  28,\n",
       "  169,\n",
       "  93,\n",
       "  168,\n",
       "  137,\n",
       "  190,\n",
       "  297,\n",
       "  259,\n",
       "  69,\n",
       "  231,\n",
       "  34,\n",
       "  231,\n",
       "  26,\n",
       "  6,\n",
       "  163,\n",
       "  135,\n",
       "  175,\n",
       "  220,\n",
       "  117,\n",
       "  320,\n",
       "  25,\n",
       "  20,\n",
       "  338,\n",
       "  6,\n",
       "  337,\n",
       "  168,\n",
       "  304,\n",
       "  121,\n",
       "  2,\n",
       "  54,\n",
       "  247,\n",
       "  8,\n",
       "  169,\n",
       "  2,\n",
       "  219,\n",
       "  143,\n",
       "  304,\n",
       "  53,\n",
       "  261,\n",
       "  307,\n",
       "  155,\n",
       "  6,\n",
       "  60,\n",
       "  169,\n",
       "  265,\n",
       "  307,\n",
       "  325,\n",
       "  307,\n",
       "  155,\n",
       "  169,\n",
       "  71,\n",
       "  319,\n",
       "  170,\n",
       "  123,\n",
       "  125,\n",
       "  200,\n",
       "  8,\n",
       "  34,\n",
       "  92,\n",
       "  302,\n",
       "  187,\n",
       "  303,\n",
       "  106,\n",
       "  6,\n",
       "  305,\n",
       "  228,\n",
       "  108,\n",
       "  103,\n",
       "  6,\n",
       "  165,\n",
       "  297,\n",
       "  192,\n",
       "  18,\n",
       "  217,\n",
       "  251,\n",
       "  8,\n",
       "  297,\n",
       "  256,\n",
       "  236,\n",
       "  168,\n",
       "  296,\n",
       "  297,\n",
       "  39,\n",
       "  34,\n",
       "  163,\n",
       "  57,\n",
       "  89,\n",
       "  225,\n",
       "  127,\n",
       "  180,\n",
       "  304,\n",
       "  6,\n",
       "  277,\n",
       "  329,\n",
       "  24,\n",
       "  120,\n",
       "  203,\n",
       "  220,\n",
       "  169,\n",
       "  230,\n",
       "  61,\n",
       "  297,\n",
       "  146,\n",
       "  244,\n",
       "  6,\n",
       "  277,\n",
       "  28,\n",
       "  220,\n",
       "  297,\n",
       "  287,\n",
       "  25,\n",
       "  296,\n",
       "  86,\n",
       "  281,\n",
       "  307,\n",
       "  187,\n",
       "  20,\n",
       "  182,\n",
       "  55,\n",
       "  220,\n",
       "  266,\n",
       "  6,\n",
       "  60,\n",
       "  169,\n",
       "  284,\n",
       "  86,\n",
       "  208,\n",
       "  297,\n",
       "  187,\n",
       "  297,\n",
       "  121,\n",
       "  28,\n",
       "  296,\n",
       "  202,\n",
       "  106,\n",
       "  8,\n",
       "  163,\n",
       "  144,\n",
       "  297,\n",
       "  58,\n",
       "  181,\n",
       "  341,\n",
       "  205,\n",
       "  180,\n",
       "  304,\n",
       "  168,\n",
       "  296,\n",
       "  347,\n",
       "  268,\n",
       "  31,\n",
       "  187,\n",
       "  292,\n",
       "  296,\n",
       "  297,\n",
       "  43,\n",
       "  168,\n",
       "  19,\n",
       "  167,\n",
       "  169,\n",
       "  19,\n",
       "  108,\n",
       "  51,\n",
       "  302,\n",
       "  38,\n",
       "  138,\n",
       "  297,\n",
       "  261,\n",
       "  238,\n",
       "  307,\n",
       "  104,\n",
       "  348,\n",
       "  342,\n",
       "  220,\n",
       "  316,\n",
       "  8,\n",
       "  163,\n",
       "  191,\n",
       "  6,\n",
       "  269,\n",
       "  193,\n",
       "  257,\n",
       "  254,\n",
       "  44,\n",
       "  130,\n",
       "  324,\n",
       "  129,\n",
       "  21,\n",
       "  11,\n",
       "  200,\n",
       "  306,\n",
       "  297,\n",
       "  204,\n",
       "  168,\n",
       "  173,\n",
       "  241,\n",
       "  178,\n",
       "  0,\n",
       "  0,\n",
       "  224,\n",
       "  6,\n",
       "  329,\n",
       "  135,\n",
       "  169,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  300,\n",
       "  38,\n",
       "  239,\n",
       "  66,\n",
       "  153,\n",
       "  34,\n",
       "  329,\n",
       "  92,\n",
       "  208,\n",
       "  176,\n",
       "  339,\n",
       "  302,\n",
       "  38,\n",
       "  8,\n",
       "  92,\n",
       "  329,\n",
       "  251,\n",
       "  210,\n",
       "  307,\n",
       "  262,\n",
       "  169,\n",
       "  231,\n",
       "  34,\n",
       "  231,\n",
       "  26,\n",
       "  18,\n",
       "  162,\n",
       "  21,\n",
       "  139,\n",
       "  321,\n",
       "  88,\n",
       "  260,\n",
       "  222,\n",
       "  131,\n",
       "  166,\n",
       "  167,\n",
       "  28,\n",
       "  220,\n",
       "  297,\n",
       "  287,\n",
       "  141,\n",
       "  94,\n",
       "  165,\n",
       "  297,\n",
       "  204,\n",
       "  18,\n",
       "  35,\n",
       "  6,\n",
       "  297,\n",
       "  289,\n",
       "  310,\n",
       "  304,\n",
       "  121,\n",
       "  44,\n",
       "  130,\n",
       "  170,\n",
       "  90,\n",
       "  34,\n",
       "  67,\n",
       "  169,\n",
       "  320,\n",
       "  298,\n",
       "  6,\n",
       "  34,\n",
       "  169,\n",
       "  270,\n",
       "  8,\n",
       "  300,\n",
       "  197,\n",
       "  3,\n",
       "  50,\n",
       "  20,\n",
       "  246,\n",
       "  83,\n",
       "  294,\n",
       "  199,\n",
       "  204,\n",
       "  165,\n",
       "  154,\n",
       "  279,\n",
       "  6,\n",
       "  60,\n",
       "  163,\n",
       "  144,\n",
       "  19,\n",
       "  297,\n",
       "  291,\n",
       "  19,\n",
       "  84,\n",
       "  296,\n",
       "  313,\n",
       "  169,\n",
       "  167,\n",
       "  20,\n",
       "  206,\n",
       "  323,\n",
       "  341,\n",
       "  182,\n",
       "  100,\n",
       "  6,\n",
       "  343,\n",
       "  187,\n",
       "  202,\n",
       "  266,\n",
       "  8,\n",
       "  297,\n",
       "  23,\n",
       "  38,\n",
       "  246,\n",
       "  142,\n",
       "  129,\n",
       "  297,\n",
       "  203,\n",
       "  236,\n",
       "  6,\n",
       "  30,\n",
       "  332,\n",
       "  52,\n",
       "  173,\n",
       "  264,\n",
       "  307,\n",
       "  47,\n",
       "  242,\n",
       "  297,\n",
       "  111,\n",
       "  259,\n",
       "  63,\n",
       "  296,\n",
       "  151,\n",
       "  86,\n",
       "  165,\n",
       "  32,\n",
       "  48,\n",
       "  6,\n",
       "  227,\n",
       "  165,\n",
       "  20,\n",
       "  212,\n",
       "  211,\n",
       "  8,\n",
       "  60,\n",
       "  207,\n",
       "  54,\n",
       "  177,\n",
       "  140,\n",
       "  230,\n",
       "  307,\n",
       "  257,\n",
       "  6,\n",
       "  339,\n",
       "  159,\n",
       "  153,\n",
       "  233,\n",
       "  306,\n",
       "  297,\n",
       "  107,\n",
       "  121,\n",
       "  6,\n",
       "  34,\n",
       "  24,\n",
       "  149,\n",
       "  347,\n",
       "  118,\n",
       "  153,\n",
       "  63,\n",
       "  2,\n",
       "  318,\n",
       "  8,\n",
       "  232,\n",
       "  6,\n",
       "  297,\n",
       "  121,\n",
       "  93,\n",
       "  208,\n",
       "  283,\n",
       "  49,\n",
       "  169,\n",
       "  93,\n",
       "  208,\n",
       "  105,\n",
       "  6,\n",
       "  169,\n",
       "  2,\n",
       "  73,\n",
       "  6,\n",
       "  169,\n",
       "  250,\n",
       "  112,\n",
       "  34,\n",
       "  169,\n",
       "  119,\n",
       "  246,\n",
       "  252,\n",
       "  129,\n",
       "  203,\n",
       "  220,\n",
       "  170,\n",
       "  255,\n",
       "  6,\n",
       "  85,\n",
       "  20,\n",
       "  246,\n",
       "  75,\n",
       "  102,\n",
       "  34,\n",
       "  115,\n",
       "  307,\n",
       "  28,\n",
       "  220,\n",
       "  297,\n",
       "  78,\n",
       "  296,\n",
       "  62,\n",
       "  51,\n",
       "  169,\n",
       "  8,\n",
       "  223,\n",
       "  6,\n",
       "  34,\n",
       "  61,\n",
       "  297,\n",
       "  327,\n",
       "  6,\n",
       "  304,\n",
       "  168,\n",
       "  217,\n",
       "  20,\n",
       "  160,\n",
       "  228,\n",
       "  294,\n",
       "  275,\n",
       "  126,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  169,\n",
       "  2,\n",
       "  173,\n",
       "  235,\n",
       "  307,\n",
       "  183,\n",
       "  296,\n",
       "  327,\n",
       "  49,\n",
       "  278,\n",
       "  168,\n",
       "  35,\n",
       "  41,\n",
       "  296,\n",
       "  297,\n",
       "  134,\n",
       "  168,\n",
       "  284,\n",
       "  161,\n",
       "  341,\n",
       "  297,\n",
       "  174,\n",
       "  8,\n",
       "  169,\n",
       "  29,\n",
       "  344,\n",
       "  249,\n",
       "  314,\n",
       "  346,\n",
       "  27,\n",
       "  34,\n",
       "  149,\n",
       "  50,\n",
       "  273,\n",
       "  225,\n",
       "  297,\n",
       "  267,\n",
       "  109,\n",
       "  272,\n",
       "  8,\n",
       "  334,\n",
       "  8,\n",
       "  8,\n",
       "  8,\n",
       "  274,\n",
       "  169,\n",
       "  0,\n",
       "  336,\n",
       "  2,\n",
       "  171,\n",
       "  70,\n",
       "  130,\n",
       "  18,\n",
       "  20,\n",
       "  213,\n",
       "  220,\n",
       "  101,\n",
       "  288,\n",
       "  12,\n",
       "  4,\n",
       "  14,\n",
       "  5,\n",
       "  7,\n",
       "  56,\n",
       "  340,\n",
       "  10,\n",
       "  4,\n",
       "  14,\n",
       "  5,\n",
       "  7,\n",
       "  297,\n",
       "  80,\n",
       "  4,\n",
       "  16,\n",
       "  5,\n",
       "  7,\n",
       "  297,\n",
       "  80,\n",
       "  17,\n",
       "  258,\n",
       "  4,\n",
       "  13,\n",
       "  5,\n",
       "  7,\n",
       "  185,\n",
       "  156,\n",
       "  4,\n",
       "  9,\n",
       "  5,\n",
       "  7,\n",
       "  195,\n",
       "  4,\n",
       "  9,\n",
       "  5,\n",
       "  7,\n",
       "  297,\n",
       "  229,\n",
       "  4,\n",
       "  16,\n",
       "  5,\n",
       "  7,\n",
       "  285,\n",
       "  220,\n",
       "  99,\n",
       "  4,\n",
       "  15,\n",
       "  5],\n",
       " 'Y': 'neg'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dataset[0]  # First review in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[243,\n",
       " 17,\n",
       " 314,\n",
       " 294,\n",
       " 77,\n",
       " 140,\n",
       " 307,\n",
       " 20,\n",
       " 68,\n",
       " 237,\n",
       " 6,\n",
       " 97,\n",
       " 34,\n",
       " 299,\n",
       " 98,\n",
       " 8,\n",
       " 302,\n",
       " 135,\n",
       " 167,\n",
       " 33,\n",
       " 22,\n",
       " 8,\n",
       " 226,\n",
       " 220,\n",
       " 297,\n",
       " 145,\n",
       " 87,\n",
       " 6,\n",
       " 60,\n",
       " 158,\n",
       " 136,\n",
       " 74,\n",
       " 307,\n",
       " 262,\n",
       " 157,\n",
       " 165,\n",
       " 153,\n",
       " 179,\n",
       " 6,\n",
       " 34,\n",
       " 149,\n",
       " 214,\n",
       " 8,\n",
       " 333,\n",
       " 2,\n",
       " 297,\n",
       " 82,\n",
       " 18,\n",
       " 326,\n",
       " 297,\n",
       " 204,\n",
       " 34,\n",
       " 19,\n",
       " 280,\n",
       " 19,\n",
       " 124,\n",
       " 230,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 79,\n",
       " 17,\n",
       " 20,\n",
       " 199,\n",
       " 204,\n",
       " 129,\n",
       " 297,\n",
       " 294,\n",
       " 133,\n",
       " 296,\n",
       " 311,\n",
       " 225,\n",
       " 20,\n",
       " 322,\n",
       " 75,\n",
       " 164,\n",
       " 6,\n",
       " 60,\n",
       " 245,\n",
       " 169,\n",
       " 165,\n",
       " 20,\n",
       " 322,\n",
       " 46,\n",
       " 234,\n",
       " 8,\n",
       " 337,\n",
       " 168,\n",
       " 333,\n",
       " 188,\n",
       " 304,\n",
       " 253,\n",
       " 33,\n",
       " 108,\n",
       " 148,\n",
       " 226,\n",
       " 307,\n",
       " 345,\n",
       " 6,\n",
       " 272,\n",
       " 163,\n",
       " 132,\n",
       " 37,\n",
       " 122,\n",
       " 337,\n",
       " 42,\n",
       " 307,\n",
       " 59,\n",
       " 297,\n",
       " 201,\n",
       " 6,\n",
       " 196,\n",
       " 341,\n",
       " 348,\n",
       " 152,\n",
       " 34,\n",
       " 290,\n",
       " 4,\n",
       " 185,\n",
       " 156,\n",
       " 1,\n",
       " 195,\n",
       " 5,\n",
       " 6,\n",
       " 60,\n",
       " 300,\n",
       " 38,\n",
       " 142,\n",
       " 34,\n",
       " 46,\n",
       " 328,\n",
       " 220,\n",
       " 189,\n",
       " 28,\n",
       " 315,\n",
       " 220,\n",
       " 122,\n",
       " 6,\n",
       " 34,\n",
       " 301,\n",
       " 128,\n",
       " 173,\n",
       " 86,\n",
       " 208,\n",
       " 276,\n",
       " 304,\n",
       " 226,\n",
       " 76,\n",
       " 8,\n",
       " 302,\n",
       " 263,\n",
       " 307,\n",
       " 150,\n",
       " 293,\n",
       " 304,\n",
       " 246,\n",
       " 209,\n",
       " 72,\n",
       " 6,\n",
       " 60,\n",
       " 113,\n",
       " 169,\n",
       " 295,\n",
       " 8,\n",
       " 277,\n",
       " 333,\n",
       " 38,\n",
       " 297,\n",
       " 248,\n",
       " 341,\n",
       " 297,\n",
       " 204,\n",
       " 18,\n",
       " 331,\n",
       " 6,\n",
       " 170,\n",
       " 186,\n",
       " 247,\n",
       " 168,\n",
       " 296,\n",
       " 169,\n",
       " 2,\n",
       " 271,\n",
       " 309,\n",
       " 172,\n",
       " 8,\n",
       " 169,\n",
       " 282,\n",
       " 221,\n",
       " 19,\n",
       " 216,\n",
       " 19,\n",
       " 60,\n",
       " 299,\n",
       " 95,\n",
       " 167,\n",
       " 304,\n",
       " 19,\n",
       " 116,\n",
       " 19,\n",
       " 342,\n",
       " 165,\n",
       " 337,\n",
       " 347,\n",
       " 6,\n",
       " 40,\n",
       " 33,\n",
       " 43,\n",
       " 194,\n",
       " 6,\n",
       " 150,\n",
       " 215,\n",
       " 164,\n",
       " 333,\n",
       " 2,\n",
       " 141,\n",
       " 225,\n",
       " 8,\n",
       " 300,\n",
       " 38,\n",
       " 96,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 64,\n",
       " 70,\n",
       " 45,\n",
       " 130,\n",
       " 297,\n",
       " 81,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 229,\n",
       " 339,\n",
       " 183,\n",
       " 180,\n",
       " 297,\n",
       " 81,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 286,\n",
       " 36,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 91,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 20,\n",
       " 184,\n",
       " 220,\n",
       " 65,\n",
       " 260,\n",
       " 6,\n",
       " 300,\n",
       " 38,\n",
       " 308,\n",
       " 220,\n",
       " 330,\n",
       " 303,\n",
       " 296,\n",
       " 147,\n",
       " 6,\n",
       " 34,\n",
       " 203,\n",
       " 220,\n",
       " 169,\n",
       " 168,\n",
       " 271,\n",
       " 217,\n",
       " 114,\n",
       " 8,\n",
       " 218,\n",
       " 163,\n",
       " 240,\n",
       " 92,\n",
       " 208,\n",
       " 198,\n",
       " 312,\n",
       " 307,\n",
       " 317,\n",
       " 20,\n",
       " 121,\n",
       " 110,\n",
       " 218,\n",
       " 34,\n",
       " 299,\n",
       " 6,\n",
       " 60,\n",
       " 335,\n",
       " 28,\n",
       " 169,\n",
       " 93,\n",
       " 168,\n",
       " 137,\n",
       " 190,\n",
       " 297,\n",
       " 259,\n",
       " 69,\n",
       " 231,\n",
       " 34,\n",
       " 231,\n",
       " 26,\n",
       " 6,\n",
       " 163,\n",
       " 135,\n",
       " 175,\n",
       " 220,\n",
       " 117,\n",
       " 320,\n",
       " 25,\n",
       " 20,\n",
       " 338,\n",
       " 6,\n",
       " 337,\n",
       " 168,\n",
       " 304,\n",
       " 121,\n",
       " 2,\n",
       " 54,\n",
       " 247,\n",
       " 8,\n",
       " 169,\n",
       " 2,\n",
       " 219,\n",
       " 143,\n",
       " 304,\n",
       " 53,\n",
       " 261,\n",
       " 307,\n",
       " 155,\n",
       " 6,\n",
       " 60,\n",
       " 169,\n",
       " 265,\n",
       " 307,\n",
       " 325,\n",
       " 307,\n",
       " 155,\n",
       " 169,\n",
       " 71,\n",
       " 319,\n",
       " 170,\n",
       " 123,\n",
       " 125,\n",
       " 200,\n",
       " 8,\n",
       " 34,\n",
       " 92,\n",
       " 302,\n",
       " 187,\n",
       " 303,\n",
       " 106,\n",
       " 6,\n",
       " 305,\n",
       " 228,\n",
       " 108,\n",
       " 103,\n",
       " 6,\n",
       " 165,\n",
       " 297,\n",
       " 192,\n",
       " 18,\n",
       " 217,\n",
       " 251,\n",
       " 8,\n",
       " 297,\n",
       " 256,\n",
       " 236,\n",
       " 168,\n",
       " 296,\n",
       " 297,\n",
       " 39,\n",
       " 34,\n",
       " 163,\n",
       " 57,\n",
       " 89,\n",
       " 225,\n",
       " 127,\n",
       " 180,\n",
       " 304,\n",
       " 6,\n",
       " 277,\n",
       " 329,\n",
       " 24,\n",
       " 120,\n",
       " 203,\n",
       " 220,\n",
       " 169,\n",
       " 230,\n",
       " 61,\n",
       " 297,\n",
       " 146,\n",
       " 244,\n",
       " 6,\n",
       " 277,\n",
       " 28,\n",
       " 220,\n",
       " 297,\n",
       " 287,\n",
       " 25,\n",
       " 296,\n",
       " 86,\n",
       " 281,\n",
       " 307,\n",
       " 187,\n",
       " 20,\n",
       " 182,\n",
       " 55,\n",
       " 220,\n",
       " 266,\n",
       " 6,\n",
       " 60,\n",
       " 169,\n",
       " 284,\n",
       " 86,\n",
       " 208,\n",
       " 297,\n",
       " 187,\n",
       " 297,\n",
       " 121,\n",
       " 28,\n",
       " 296,\n",
       " 202,\n",
       " 106,\n",
       " 8,\n",
       " 163,\n",
       " 144,\n",
       " 297,\n",
       " 58,\n",
       " 181,\n",
       " 341,\n",
       " 205,\n",
       " 180,\n",
       " 304,\n",
       " 168,\n",
       " 296,\n",
       " 347,\n",
       " 268,\n",
       " 31,\n",
       " 187,\n",
       " 292,\n",
       " 296,\n",
       " 297,\n",
       " 43,\n",
       " 168,\n",
       " 19,\n",
       " 167,\n",
       " 169,\n",
       " 19,\n",
       " 108,\n",
       " 51,\n",
       " 302,\n",
       " 38,\n",
       " 138,\n",
       " 297,\n",
       " 261,\n",
       " 238,\n",
       " 307,\n",
       " 104,\n",
       " 348,\n",
       " 342,\n",
       " 220,\n",
       " 316,\n",
       " 8,\n",
       " 163,\n",
       " 191,\n",
       " 6,\n",
       " 269,\n",
       " 193,\n",
       " 257,\n",
       " 254,\n",
       " 44,\n",
       " 130,\n",
       " 324,\n",
       " 129,\n",
       " 21,\n",
       " 11,\n",
       " 200,\n",
       " 306,\n",
       " 297,\n",
       " 204,\n",
       " 168,\n",
       " 173,\n",
       " 241,\n",
       " 178,\n",
       " 0,\n",
       " 0,\n",
       " 224,\n",
       " 6,\n",
       " 329,\n",
       " 135,\n",
       " 169,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 300,\n",
       " 38,\n",
       " 239,\n",
       " 66,\n",
       " 153,\n",
       " 34,\n",
       " 329,\n",
       " 92,\n",
       " 208,\n",
       " 176,\n",
       " 339,\n",
       " 302,\n",
       " 38,\n",
       " 8,\n",
       " 92,\n",
       " 329,\n",
       " 251,\n",
       " 210,\n",
       " 307,\n",
       " 262,\n",
       " 169,\n",
       " 231,\n",
       " 34,\n",
       " 231,\n",
       " 26,\n",
       " 18,\n",
       " 162,\n",
       " 21,\n",
       " 139,\n",
       " 321,\n",
       " 88,\n",
       " 260,\n",
       " 222,\n",
       " 131,\n",
       " 166,\n",
       " 167,\n",
       " 28,\n",
       " 220,\n",
       " 297,\n",
       " 287,\n",
       " 141,\n",
       " 94,\n",
       " 165,\n",
       " 297,\n",
       " 204,\n",
       " 18,\n",
       " 35,\n",
       " 6,\n",
       " 297,\n",
       " 289,\n",
       " 310,\n",
       " 304,\n",
       " 121,\n",
       " 44,\n",
       " 130,\n",
       " 170,\n",
       " 90,\n",
       " 34,\n",
       " 67,\n",
       " 169,\n",
       " 320,\n",
       " 298,\n",
       " 6,\n",
       " 34,\n",
       " 169,\n",
       " 270,\n",
       " 8,\n",
       " 300,\n",
       " 197,\n",
       " 3,\n",
       " 50,\n",
       " 20,\n",
       " 246,\n",
       " 83,\n",
       " 294,\n",
       " 199,\n",
       " 204,\n",
       " 165,\n",
       " 154,\n",
       " 279,\n",
       " 6,\n",
       " 60,\n",
       " 163,\n",
       " 144,\n",
       " 19,\n",
       " 297,\n",
       " 291,\n",
       " 19,\n",
       " 84,\n",
       " 296,\n",
       " 313,\n",
       " 169,\n",
       " 167,\n",
       " 20,\n",
       " 206,\n",
       " 323,\n",
       " 341,\n",
       " 182,\n",
       " 100,\n",
       " 6,\n",
       " 343,\n",
       " 187,\n",
       " 202,\n",
       " 266,\n",
       " 8,\n",
       " 297,\n",
       " 23,\n",
       " 38,\n",
       " 246,\n",
       " 142,\n",
       " 129,\n",
       " 297,\n",
       " 203,\n",
       " 236,\n",
       " 6,\n",
       " 30,\n",
       " 332,\n",
       " 52,\n",
       " 173,\n",
       " 264,\n",
       " 307,\n",
       " 47,\n",
       " 242,\n",
       " 297,\n",
       " 111,\n",
       " 259,\n",
       " 63,\n",
       " 296,\n",
       " 151,\n",
       " 86,\n",
       " 165,\n",
       " 32,\n",
       " 48,\n",
       " 6,\n",
       " 227,\n",
       " 165,\n",
       " 20,\n",
       " 212,\n",
       " 211,\n",
       " 8,\n",
       " 60,\n",
       " 207,\n",
       " 54,\n",
       " 177,\n",
       " 140,\n",
       " 230,\n",
       " 307,\n",
       " 257,\n",
       " 6,\n",
       " 339,\n",
       " 159,\n",
       " 153,\n",
       " 233,\n",
       " 306,\n",
       " 297,\n",
       " 107,\n",
       " 121,\n",
       " 6,\n",
       " 34,\n",
       " 24,\n",
       " 149,\n",
       " 347,\n",
       " 118,\n",
       " 153,\n",
       " 63,\n",
       " 2,\n",
       " 318,\n",
       " 8,\n",
       " 232,\n",
       " 6,\n",
       " 297,\n",
       " 121,\n",
       " 93,\n",
       " 208,\n",
       " 283,\n",
       " 49,\n",
       " 169,\n",
       " 93,\n",
       " 208,\n",
       " 105,\n",
       " 6,\n",
       " 169,\n",
       " 2,\n",
       " 73,\n",
       " 6,\n",
       " 169,\n",
       " 250,\n",
       " 112,\n",
       " 34,\n",
       " 169,\n",
       " 119,\n",
       " 246,\n",
       " 252,\n",
       " 129,\n",
       " 203,\n",
       " 220,\n",
       " 170,\n",
       " 255,\n",
       " 6,\n",
       " 85,\n",
       " 20,\n",
       " 246,\n",
       " 75,\n",
       " 102,\n",
       " 34,\n",
       " 115,\n",
       " 307,\n",
       " 28,\n",
       " 220,\n",
       " 297,\n",
       " 78,\n",
       " 296,\n",
       " 62,\n",
       " 51,\n",
       " 169,\n",
       " 8,\n",
       " 223,\n",
       " 6,\n",
       " 34,\n",
       " 61,\n",
       " 297,\n",
       " 327,\n",
       " 6,\n",
       " 304,\n",
       " 168,\n",
       " 217,\n",
       " 20,\n",
       " 160,\n",
       " 228,\n",
       " 294,\n",
       " 275,\n",
       " 126,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 169,\n",
       " 2,\n",
       " 173,\n",
       " 235,\n",
       " 307,\n",
       " 183,\n",
       " 296,\n",
       " 327,\n",
       " 49,\n",
       " 278,\n",
       " 168,\n",
       " 35,\n",
       " 41,\n",
       " 296,\n",
       " 297,\n",
       " 134,\n",
       " 168,\n",
       " 284,\n",
       " 161,\n",
       " 341,\n",
       " 297,\n",
       " 174,\n",
       " 8,\n",
       " 169,\n",
       " 29,\n",
       " 344,\n",
       " 249,\n",
       " 314,\n",
       " 346,\n",
       " 27,\n",
       " 34,\n",
       " 149,\n",
       " 50,\n",
       " 273,\n",
       " 225,\n",
       " 297,\n",
       " 267,\n",
       " 109,\n",
       " 272,\n",
       " 8,\n",
       " 334,\n",
       " 8,\n",
       " 8,\n",
       " 8,\n",
       " 274,\n",
       " 169,\n",
       " 0,\n",
       " 336,\n",
       " 2,\n",
       " 171,\n",
       " 70,\n",
       " 130,\n",
       " 18,\n",
       " 20,\n",
       " 213,\n",
       " 220,\n",
       " 101,\n",
       " 288,\n",
       " 12,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 56,\n",
       " 340,\n",
       " 10,\n",
       " 4,\n",
       " 14,\n",
       " 5,\n",
       " 7,\n",
       " 297,\n",
       " 80,\n",
       " 4,\n",
       " 16,\n",
       " 5,\n",
       " 7,\n",
       " 297,\n",
       " 80,\n",
       " 17,\n",
       " 258,\n",
       " 4,\n",
       " 13,\n",
       " 5,\n",
       " 7,\n",
       " 185,\n",
       " 156,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 195,\n",
       " 4,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 297,\n",
       " 229,\n",
       " 4,\n",
       " 16,\n",
       " 5,\n",
       " 7,\n",
       " 285,\n",
       " 220,\n",
       " 99,\n",
       " 4,\n",
       " 15,\n",
       " 5]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_dataset[0]['X']  # Label of the first review in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1\"></a>\n",
    "\n",
    "# 3.1 Word2Vec Training\n",
    "\n",
    "Word2Vec has two training variants:\n",
    "\n",
    " - **Continuous Bag of Words (CBOW)**: Predict center word from (bag of) context words.\n",
    " - **Skip-grams**: Predict context words given center word.\n",
    "  \n",
    "Visually, they look like this:\n",
    "\n",
    "<img src=\"https://ibin.co/4UIznsOEyH7t.png\" width=\"500\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-1\"></a>\n",
    "\n",
    "## 3.1.1. CBOW\n",
    "\n",
    "CBOW windows through the sentence and picks out the center word as the `Y` and the surrounding context words as the inputs `X`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (4, None, None)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lazyme import per_window, per_chunk\n",
    "\n",
    "xx =[1,2,3,4]\n",
    "list(per_window(xx, n=2))\n",
    "list(per_chunk(xx, n=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_window(sequence, n=1):\n",
    "    \"\"\"\n",
    "    From http://stackoverflow.com/q/42220614/610569\n",
    "        >>> list(per_window([1,2,3,4], n=2))\n",
    "        [(1, 2), (2, 3), (3, 4)]\n",
    "        >>> list(per_window([1,2,3,4], n=3))\n",
    "        [(1, 2, 3), (2, 3, 4)]\n",
    "    \"\"\"\n",
    "    start, stop = 0, n\n",
    "    seq = list(sequence)\n",
    "    while stop <= len(seq):\n",
    "        yield seq[start:stop]\n",
    "        start += 1\n",
    "        stop += 1\n",
    "\n",
    "def cbow_iterator(tokens, window_size):\n",
    "    n = window_size * 2 + 1\n",
    "    for window in per_window(tokens, n):\n",
    "        target = window.pop(window_size)\n",
    "        yield window, target   # X = window ; Y = target. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent0 = ['language', 'users', 'never', 'choose', 'words', 'randomly', ',', \n",
    "         'and', 'language', 'is', 'essentially', 'non-random', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['language', 'users', 'choose', 'words'], 'never'),\n",
       " (['users', 'never', 'words', 'randomly'], 'choose'),\n",
       " (['never', 'choose', 'randomly', ','], 'words'),\n",
       " (['choose', 'words', ',', 'and'], 'randomly'),\n",
       " (['words', 'randomly', 'and', 'language'], ','),\n",
       " (['randomly', ',', 'language', 'is'], 'and'),\n",
       " ([',', 'and', 'is', 'essentially'], 'language'),\n",
       " (['and', 'language', 'essentially', 'non-random'], 'is'),\n",
       " (['language', 'is', 'non-random', '.'], 'essentially')]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cbow_iterator(sent0, 2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['language', 'users', 'never', 'words', 'randomly', ','], 'choose'),\n",
       " (['users', 'never', 'choose', 'randomly', ',', 'and'], 'words'),\n",
       " (['never', 'choose', 'words', ',', 'and', 'language'], 'randomly'),\n",
       " (['choose', 'words', 'randomly', 'and', 'language', 'is'], ','),\n",
       " (['words', 'randomly', ',', 'language', 'is', 'essentially'], 'and'),\n",
       " (['randomly', ',', 'and', 'is', 'essentially', 'non-random'], 'language'),\n",
       " ([',', 'and', 'language', 'essentially', 'non-random', '.'], 'is')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cbow_iterator(sent0, 3)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-2\"></a>\n",
    "\n",
    "## 3.1.2. Skipgram\n",
    "\n",
    "Skipgram training windows through the sentence and pictures out the center word as the input `X` and the context words as the outputs `Y`, additionally, it will randommly sample words not in the window as **negative samples**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram_iterator(tokens, window_size):\n",
    "    n = window_size * 2 + 1 \n",
    "    for i, window in enumerate(per_window(tokens, n)):\n",
    "        target = window.pop(window_size)\n",
    "        # Generate positive samples.\n",
    "        for context_word in window:\n",
    "            yield target, context_word, 1\n",
    "        # Generate negative samples.\n",
    "        for _ in range(n-1):\n",
    "            leftovers = tokens[:i] + tokens[i+n:]\n",
    "            yield target, random.choice(leftovers), 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('never', 'language', 1),\n",
       " ('never', 'users', 1),\n",
       " ('never', 'choose', 1),\n",
       " ('never', 'words', 1),\n",
       " ('never', 'essentially', 0),\n",
       " ('never', 'is', 0),\n",
       " ('never', 'randomly', 0),\n",
       " ('never', '.', 0),\n",
       " ('choose', 'users', 1),\n",
       " ('choose', 'never', 1),\n",
       " ('choose', 'words', 1),\n",
       " ('choose', 'randomly', 1),\n",
       " ('choose', ',', 0),\n",
       " ('choose', 'and', 0),\n",
       " ('choose', 'language', 0),\n",
       " ('choose', 'language', 0),\n",
       " ('words', 'never', 1),\n",
       " ('words', 'choose', 1),\n",
       " ('words', 'randomly', 1),\n",
       " ('words', ',', 1),\n",
       " ('words', 'non-random', 0),\n",
       " ('words', 'essentially', 0),\n",
       " ('words', 'language', 0),\n",
       " ('words', 'language', 0),\n",
       " ('randomly', 'choose', 1),\n",
       " ('randomly', 'words', 1),\n",
       " ('randomly', ',', 1),\n",
       " ('randomly', 'and', 1),\n",
       " ('randomly', 'essentially', 0),\n",
       " ('randomly', 'essentially', 0),\n",
       " ('randomly', 'users', 0),\n",
       " ('randomly', 'language', 0),\n",
       " (',', 'words', 1),\n",
       " (',', 'randomly', 1),\n",
       " (',', 'and', 1),\n",
       " (',', 'language', 1),\n",
       " (',', '.', 0),\n",
       " (',', 'users', 0),\n",
       " (',', 'non-random', 0),\n",
       " (',', 'users', 0),\n",
       " ('and', 'randomly', 1),\n",
       " ('and', ',', 1),\n",
       " ('and', 'language', 1),\n",
       " ('and', 'is', 1),\n",
       " ('and', 'words', 0),\n",
       " ('and', '.', 0),\n",
       " ('and', 'words', 0),\n",
       " ('and', 'essentially', 0),\n",
       " ('language', ',', 1),\n",
       " ('language', 'and', 1),\n",
       " ('language', 'is', 1),\n",
       " ('language', 'essentially', 1),\n",
       " ('language', 'language', 0),\n",
       " ('language', 'choose', 0),\n",
       " ('language', 'randomly', 0),\n",
       " ('language', 'users', 0),\n",
       " ('is', 'and', 1),\n",
       " ('is', 'language', 1),\n",
       " ('is', 'essentially', 1),\n",
       " ('is', 'non-random', 1),\n",
       " ('is', ',', 0),\n",
       " ('is', 'choose', 0),\n",
       " ('is', 'users', 0),\n",
       " ('is', 'never', 0),\n",
       " ('essentially', 'language', 1),\n",
       " ('essentially', 'is', 1),\n",
       " ('essentially', 'non-random', 1),\n",
       " ('essentially', '.', 1),\n",
       " ('essentially', 'choose', 0),\n",
       " ('essentially', 'and', 0),\n",
       " ('essentially', 'words', 0),\n",
       " ('essentially', ',', 0)]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(skipgram_iterator(sent0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut-away: What is `partial`?\n",
    "\n",
    "The [`functools.partial`](https://docs.python.org/3.7/library/functools.html#functools.partial) function in Python is a mechanism to overload a function with preset arguments. \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'is'), ('is', 'a'), ('a', 'sentence')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "# Generates bigrams\n",
    "list(ngrams('this is a sentence'.split(), n=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# You can create a new function that \"preset\" the `n` argument, e.g.\n",
    "bigrams = partial(ngrams, n=2)\n",
    "trigrams = partial(ngrams, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'is', 'a'), ('is', 'a', 'sentence')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trigrams('this is a sentence'.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('this', 'is'), ('is', 'a'), ('a', 'sentence')]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigrams('this is a sentence'.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-3\"></a>\n",
    "\n",
    "## 3.1.3 Word2Vec Dataset\n",
    "\n",
    "Now that we know what are the inputs `X` and outputs `Y` of the Word2Vec task. \n",
    "\n",
    "Lets put everything together and modify the `Dataset` so that `__getitem__` retrieves CBOW or Skipgram formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "\n",
    "    def cbow_iterator(self,tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield window, target   # X = window ; Y = target. \n",
    "\n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            target = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield target, context_word, 1\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                yield target, random.choice(leftovers), 0\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-3-hint\"></a>\n",
    "## Hints for the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: To see the hint and partial code for the cell above, uncomment the following line.\n",
    "##hint_word2vec_dataset()\n",
    "\n",
    "# Option 2: \"I give up just, run the code for me\" \n",
    "# Uncomment the next two lines, if you really gave up... \n",
    "##full_code_word2vec_dataset()\n",
    "##from tsundoku.word2vec import Word2VecText\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-4-hint\"></a>\n",
    "\n",
    "## 3.1.4. Train a CBOW model\n",
    "\n",
    "### Lets Get Some Data\n",
    "\n",
    "Lets take Kilgarriff (2005) , \"Language is never ever, ever random\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import io #codecs\n",
    "\n",
    "\n",
    "# Text version of https://kilgarriff.co.uk/Publications/2005-K-lineer.pdf\n",
    "if os.path.isfile('language-never-random.txt'):\n",
    "    with io.open('language-never-random.txt', encoding='utf8') as fin:\n",
    "        text = fin.read()\n",
    "else:\n",
    "    url = \"https://gist.githubusercontent.com/alvations/53b01e4076573fea47c6057120bb017a/raw/b01ff96a5f76848450e648f35da6497ca9454e4a/language-never-random.txt\"\n",
    "    text = requests.get(url).content.decode('utf8')\n",
    "    with io.open('language-never-random.txt', 'w', encoding='utf8') as fout:\n",
    "        fout.write(text)\n",
    "\n",
    "tokenized_text = [list(map(str.lower, word_tokenize(sent))) for sent in sent_tokenize(text)]\n",
    "window_size = 2\n",
    "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Language is never, ever, ever, random\n",
      "\n",
      "                                                               ADAM KILGARRIFF\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Abstract\n",
      "Language users never choose words randomly, and language is essentially\n",
      "non-random. Statistical hypothesis testing uses a null hypothesis, which\n",
      "posits randomness. Hence, when we look at linguistic phenomena in cor-\n",
      "pora, the null hypothesis will never be true. Moreover, where there is enough\n",
      "data, we shall (almost) always be able to establish that it is not true. In\n",
      "corpus studies, we frequently do have enough data, so the fact that a rela-\n",
      "tion between two phenomena is demonstrably non-random, does not sup-\n",
      "port the inference that it is not arbitrary. We present experimental evidence\n",
      "of how arbitrary associations between word frequencies and corpora are\n",
      "systematically non-random. We review literature in which hypothesis test-\n",
      "ing has been used, and show how it has often led to unhelpful or mislead-\n",
      "ing results.\n",
      "Keywords: 쎲쎲쎲\n",
      "\n",
      "1. Int\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'is', 'never', ',', 'ever', ',', 'ever', ',', 'random', 'adam', 'kilgarriff', 'abstract', 'language', 'users', 'never', 'choose', 'words', 'randomly', ',', 'and', 'language', 'is', 'essentially', 'non-random', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sanity check, lets take a look at the data.\n",
    "print(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the bulk of linguistic questions concern the dis- tinction between a and m. a linguistic account of a phenomenon gen- erally gives us reason to view the relation between , for example , a verb ’ s syntax and its semantics , as motivated rather than arbitrary .\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-5989f9e2cfd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw2v_io\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mw2v_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msent_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2v_io\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mvisualize_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2v_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "from lazyme import color_str\n",
    "\n",
    "def visualize_predictions(x, y, prediction, vocab, window_size, unk='<unk>'):\n",
    "    left = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[:window_size]])\n",
    "    right = ' '.join([vocab.get(int(_x), '<unk>') for _x in x[window_size:]])\n",
    "    target = vocab.get(int(y), '<unk>')\n",
    "\n",
    "    if not prediction:\n",
    "        predicted_word = '______'\n",
    "    else:\n",
    "        predicted_word = vocab.get(int(prediction), '<unk>') \n",
    "    print(color_str(target, 'green'), '\\t' if len(target) > 6 else '\\t\\t', \n",
    "          left, color_str(predicted_word, 'green' if target == predicted_word else 'red'), right)\n",
    "    \n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "sent_idx = 10\n",
    "window_size = 2\n",
    "w2v_dataset = Word2VecText(tokenized_text, window_size=window_size, variant='cbow')\n",
    "print(' '.join(w2v_dataset.sents[sent_idx]))\n",
    "for w2v_io in w2v_dataset[sent_idx]:\n",
    "    context, target = w2v_io\n",
    "    context, target = tensor(context).to(device), tensor(target).to(device)\n",
    "    visualize_predictions(context, target, None, w2v_dataset.vocab, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-4-cbow-model\"></a>\n",
    "\n",
    "## Fill-in the code for the CBOW Model\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/word2vec-cbow.png\" width=\"500\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Image from https://lilianweng.github.io/lil-log/2017/10/15/learning-word-embedding.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets take a closer look from the inputs to the first `nn.Linear`\n",
    "\n",
    "Cos after it reach the first `nn.Linear` it's just the same as our multi-layered perceptron example =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-2141c776bc24>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Lets take a look at the first output.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mw2v_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mw2v_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: tuple indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "# Lets take a look at the first output.\n",
    "x, y = w2v_dataset[0][0]['x'],  w2v_dataset[0][0]['y'], \n",
    "\n",
    "x = tensor(x)\n",
    "y = autograd.Variable(tensor(y, dtype=torch.long))\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weight',\n",
       "              tensor([[-4.2168e-01,  4.8609e-02, -2.1194e+00, -6.1581e-01, -4.4611e-01],\n",
       "                      [ 2.1553e-01,  8.2212e-01,  7.6828e-01, -1.9757e+00,  1.0063e+00],\n",
       "                      [ 1.1148e+00, -1.5603e+00,  9.6289e-02,  1.1540e+00,  5.2998e-03],\n",
       "                      [ 1.0295e+00, -5.8385e-01,  8.4793e-01,  1.1187e+00, -7.8351e-02],\n",
       "                      [-6.6948e-01, -8.0136e-01,  1.3363e+00,  7.1023e-01, -8.5575e-01],\n",
       "                      [-1.2863e+00,  2.8408e-01, -1.0605e+00, -1.6178e+00,  8.6683e-02],\n",
       "                      [-1.2780e+00, -2.1280e+00,  3.8221e-01,  9.6016e-01,  7.6150e-01],\n",
       "                      [-1.3492e+00, -1.1232e+00, -6.3847e-01,  1.3816e+00, -2.7207e-01],\n",
       "                      [-9.0710e-01,  1.7912e+00,  1.5406e+00,  1.7876e+00, -5.0253e-01],\n",
       "                      [-8.6535e-02, -2.5198e+00,  4.1283e-02, -1.8119e+00, -4.9352e-01],\n",
       "                      [ 6.7914e-01, -5.3780e-01,  6.0494e-01,  2.9811e-01, -1.4980e+00],\n",
       "                      [-1.7107e+00, -1.1553e-02,  9.6819e-01,  5.5295e-01, -1.2484e-01],\n",
       "                      [ 1.2672e+00, -1.4675e+00,  5.1901e-01, -1.4572e+00,  1.1917e+00],\n",
       "                      [-6.3780e-01, -1.0528e+00,  1.1086e+00, -3.5323e-02,  2.4747e-01],\n",
       "                      [-3.9130e-01,  2.2906e-01,  1.7624e+00, -1.7548e+00,  2.1956e+00],\n",
       "                      [-4.8073e-01,  1.5946e+00,  3.9527e-01,  5.9376e-01,  1.1416e+00],\n",
       "                      [-1.5070e-01,  9.2504e-01, -1.4549e+00, -8.0897e-02,  1.7371e+00],\n",
       "                      [-1.1172e+00,  9.2408e-01,  5.2210e-01,  7.2980e-03,  1.3838e+00],\n",
       "                      [ 1.4765e+00,  7.2333e-01, -1.8083e-01,  9.8262e-02, -1.6190e-01],\n",
       "                      [-3.0961e-01, -1.4719e-01,  4.0798e-01, -1.0417e+00,  7.6366e-01],\n",
       "                      [-1.5487e+00,  4.6133e-02, -1.5881e+00,  6.4113e-01, -2.1324e-02],\n",
       "                      [ 2.1481e+00,  2.9574e-01,  5.1518e-01, -1.1093e+00,  1.2520e+00],\n",
       "                      [ 1.6027e-01, -7.8397e-01,  7.9158e-01,  1.4137e+00, -8.6596e-02],\n",
       "                      [ 3.4649e-01,  5.4315e-02,  2.8082e-01,  1.4626e+00,  2.0788e-01],\n",
       "                      [-1.0310e+00,  5.8431e-01, -1.9876e-01, -7.4032e-01,  2.3494e-02],\n",
       "                      [ 4.6759e-01, -6.0317e-01,  3.0796e-01,  6.3031e-01, -1.3129e+00],\n",
       "                      [-1.4697e-01, -1.6053e+00,  4.6007e-01, -1.3819e-01,  6.1302e-01],\n",
       "                      [-8.6968e-02,  8.2363e-01,  1.5689e+00, -7.5890e-01, -5.7329e-01],\n",
       "                      [ 5.5448e-01,  4.9221e-01, -1.9984e-02,  1.2856e+00, -6.8700e-01],\n",
       "                      [-5.0862e-01,  1.7363e+00, -7.4993e-02, -1.5138e+00, -1.6022e-01],\n",
       "                      [ 2.6567e-01,  2.3749e-03, -9.6876e-01, -1.2568e+00, -4.6934e-01],\n",
       "                      [-2.4214e-01,  6.4046e-01, -4.7389e-01, -1.6566e+00, -4.2344e-01],\n",
       "                      [ 1.3637e+00, -6.5284e-03,  8.3178e-01, -1.2724e+00, -1.4238e-01],\n",
       "                      [-8.1988e-02,  1.9979e+00,  8.8980e-01, -2.5069e-01,  9.0310e-01],\n",
       "                      [-1.5832e-01, -7.7591e-01,  1.3756e+00, -3.0197e+00,  1.4445e+00],\n",
       "                      [ 2.0460e+00, -5.9102e-01, -1.9290e+00,  1.2833e+00, -3.2033e-02],\n",
       "                      [-2.2499e+00, -1.2385e+00,  1.1165e+00, -6.7748e-01, -2.8059e-01],\n",
       "                      [ 1.0942e+00,  3.7003e-01,  4.1162e-01, -1.1308e+00,  7.5565e-01],\n",
       "                      [ 4.3888e-01,  7.1593e-01, -4.9391e-01, -2.5323e+00,  1.5953e-01],\n",
       "                      [ 1.8021e+00, -5.3685e-02,  1.6552e-01, -1.0878e-01, -4.7902e-01],\n",
       "                      [-1.3238e+00, -7.3092e-01,  1.2495e-01, -5.1265e-01, -2.1001e-02],\n",
       "                      [ 8.1081e-01,  2.1571e+00, -9.0536e-01, -9.7797e-01,  1.0033e+00],\n",
       "                      [ 1.5960e-01, -1.9311e+00,  9.3730e-01,  1.2225e+00,  6.9969e-01],\n",
       "                      [ 7.3823e-01,  2.2812e-01, -1.2195e+00,  1.5090e+00,  1.0075e+00],\n",
       "                      [-8.2614e-01, -1.1869e+00,  2.1936e-01, -8.1172e-01, -2.7830e-01],\n",
       "                      [-1.1902e-01, -3.0696e-01,  2.3452e-01, -4.1017e-01, -1.2077e-03],\n",
       "                      [ 3.3799e-01, -7.1143e-01, -2.9996e-02,  1.1469e+00, -3.4641e+00],\n",
       "                      [-1.0377e+00, -1.0576e+00,  9.9926e-01,  6.6487e-01,  3.7016e-01],\n",
       "                      [-1.1225e+00, -7.2117e-01, -5.8179e-01, -1.1653e+00,  9.6969e-01],\n",
       "                      [ 1.0698e+00,  1.2511e+00,  1.5606e+00, -1.7587e+00,  9.2387e-01],\n",
       "                      [ 4.6884e-01,  1.4788e+00, -1.1199e+00, -4.8899e-01, -3.3004e-01],\n",
       "                      [ 5.8784e-02, -8.3484e-01,  9.7435e-01, -7.0232e-01,  1.7969e-01],\n",
       "                      [-1.3162e+00,  6.9337e-01, -2.3976e-01, -6.4308e-01, -2.6167e+00],\n",
       "                      [-1.6908e-01,  5.7425e-01,  1.1514e+00,  1.4980e+00,  7.6220e-01],\n",
       "                      [-1.0870e-01,  9.8064e-02,  1.2876e+00,  7.0361e-01,  9.7530e-01],\n",
       "                      [ 1.8097e-01, -7.9905e-01,  1.5082e+00,  6.2830e-01,  1.0360e+00],\n",
       "                      [ 6.4196e-01, -1.4848e+00, -3.8311e-01, -4.9405e-01,  1.0312e+00],\n",
       "                      [ 1.7791e+00,  6.5781e-01, -3.7001e-01, -2.3752e-01,  7.3138e-01],\n",
       "                      [ 8.1269e-01, -2.2881e-01,  2.1662e-01,  6.0128e-01,  9.0422e-01],\n",
       "                      [-2.5421e+00,  5.3441e-02,  3.7462e-02, -1.0307e+00,  8.9576e-01],\n",
       "                      [-8.8123e-01,  1.1422e+00, -3.4259e-01,  1.4839e+00,  1.9674e+00],\n",
       "                      [-1.1887e-01, -7.1068e-01,  9.2613e-01,  1.0187e+00, -1.0274e+00],\n",
       "                      [ 1.1371e+00, -1.5404e-02,  2.2217e+00,  4.7553e-01,  4.0265e-01],\n",
       "                      [-1.9547e-04, -2.7674e-02, -5.1529e-01,  7.0663e-02,  1.3232e+00],\n",
       "                      [-1.4967e+00,  6.7566e-01, -1.4665e+00, -6.0045e-02, -1.5478e+00],\n",
       "                      [-4.2019e-01, -5.6151e-01,  7.9029e-03,  2.4482e-01,  1.4311e-01],\n",
       "                      [ 2.5649e-01,  7.4082e-02, -1.1038e+00,  6.2298e-01, -1.4735e+00],\n",
       "                      [-5.7956e-01, -5.1109e-01, -3.9810e-01,  5.2610e-01, -6.0173e-01],\n",
       "                      [-8.1252e-01,  9.5610e-01,  4.1863e-01, -1.0786e-01, -4.5301e-01],\n",
       "                      [ 3.9820e-02, -4.9150e-01,  2.2728e-01, -2.0722e+00,  9.0503e-01],\n",
       "                      [ 1.3834e+00, -7.9495e-01,  3.5242e-01, -3.1705e-01,  1.5981e+00],\n",
       "                      [ 3.4150e-01, -7.4918e-01, -1.3994e-01,  2.8368e-01,  1.8487e+00],\n",
       "                      [ 4.7278e-01,  8.6713e-01,  9.4905e-01,  1.4957e+00, -5.2610e-01],\n",
       "                      [-7.4715e-01, -1.8114e-01,  7.5122e-01,  3.0588e-01, -7.2012e-01],\n",
       "                      [-5.7675e-01, -7.6974e-01, -4.5740e-01, -3.4532e-01, -1.0068e-01],\n",
       "                      [-1.1889e+00, -5.1689e-01,  1.7264e-01,  7.7513e-02, -3.2247e-01],\n",
       "                      [ 1.8926e-01,  8.2962e-01,  2.2946e-01, -6.8168e-01, -1.3918e+00],\n",
       "                      [-1.4143e-01,  7.5913e-02, -1.3531e+00, -3.8334e-01,  1.0837e+00],\n",
       "                      [ 5.8596e-01,  8.6788e-01, -6.7888e-01,  1.0356e+00, -3.5321e-02],\n",
       "                      [-6.4663e-01, -1.5173e+00,  4.7762e-02, -6.7670e-01, -3.8074e-01],\n",
       "                      [-2.4082e-01,  1.1621e+00,  9.3939e-01,  1.5396e+00, -1.1659e+00],\n",
       "                      [ 3.7724e-01, -1.1616e-01, -1.0658e+00, -5.9973e-02,  3.5084e-01],\n",
       "                      [ 8.3390e-01,  2.0714e-01,  9.4343e-01,  9.1990e-01, -7.0812e-01],\n",
       "                      [ 9.3557e-01, -1.1274e+00,  9.2009e-02,  1.1264e+00, -1.1937e+00],\n",
       "                      [-1.2040e+00, -2.7830e-01,  2.7153e-01,  2.2591e-02, -5.2719e-01],\n",
       "                      [-7.5565e-01,  3.8394e-01, -4.5701e-01,  9.5821e-02,  6.5855e-01],\n",
       "                      [ 8.3232e-01,  4.4993e-01, -1.9792e+00, -7.8102e-02, -4.9347e-01]]))])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embd_size = 5\n",
    "emb = nn.Embedding(len(w2v_dataset.vocab), embd_size)\n",
    "emb.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([87, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-4.2168e-01,  4.8609e-02, -2.1194e+00, -6.1581e-01, -4.4611e-01],\n",
       "        [ 2.1553e-01,  8.2212e-01,  7.6828e-01, -1.9757e+00,  1.0063e+00],\n",
       "        [ 1.1148e+00, -1.5603e+00,  9.6289e-02,  1.1540e+00,  5.2998e-03],\n",
       "        [ 1.0295e+00, -5.8385e-01,  8.4793e-01,  1.1187e+00, -7.8351e-02],\n",
       "        [-6.6948e-01, -8.0136e-01,  1.3363e+00,  7.1023e-01, -8.5575e-01],\n",
       "        [-1.2863e+00,  2.8408e-01, -1.0605e+00, -1.6178e+00,  8.6683e-02],\n",
       "        [-1.2780e+00, -2.1280e+00,  3.8221e-01,  9.6016e-01,  7.6150e-01],\n",
       "        [-1.3492e+00, -1.1232e+00, -6.3847e-01,  1.3816e+00, -2.7207e-01],\n",
       "        [-9.0710e-01,  1.7912e+00,  1.5406e+00,  1.7876e+00, -5.0253e-01],\n",
       "        [-8.6535e-02, -2.5198e+00,  4.1283e-02, -1.8119e+00, -4.9352e-01],\n",
       "        [ 6.7914e-01, -5.3780e-01,  6.0494e-01,  2.9811e-01, -1.4980e+00],\n",
       "        [-1.7107e+00, -1.1553e-02,  9.6819e-01,  5.5295e-01, -1.2484e-01],\n",
       "        [ 1.2672e+00, -1.4675e+00,  5.1901e-01, -1.4572e+00,  1.1917e+00],\n",
       "        [-6.3780e-01, -1.0528e+00,  1.1086e+00, -3.5323e-02,  2.4747e-01],\n",
       "        [-3.9130e-01,  2.2906e-01,  1.7624e+00, -1.7548e+00,  2.1956e+00],\n",
       "        [-4.8073e-01,  1.5946e+00,  3.9527e-01,  5.9376e-01,  1.1416e+00],\n",
       "        [-1.5070e-01,  9.2504e-01, -1.4549e+00, -8.0897e-02,  1.7371e+00],\n",
       "        [-1.1172e+00,  9.2408e-01,  5.2210e-01,  7.2980e-03,  1.3838e+00],\n",
       "        [ 1.4765e+00,  7.2333e-01, -1.8083e-01,  9.8262e-02, -1.6190e-01],\n",
       "        [-3.0961e-01, -1.4719e-01,  4.0798e-01, -1.0417e+00,  7.6366e-01],\n",
       "        [-1.5487e+00,  4.6133e-02, -1.5881e+00,  6.4113e-01, -2.1324e-02],\n",
       "        [ 2.1481e+00,  2.9574e-01,  5.1518e-01, -1.1093e+00,  1.2520e+00],\n",
       "        [ 1.6027e-01, -7.8397e-01,  7.9158e-01,  1.4137e+00, -8.6596e-02],\n",
       "        [ 3.4649e-01,  5.4315e-02,  2.8082e-01,  1.4626e+00,  2.0788e-01],\n",
       "        [-1.0310e+00,  5.8431e-01, -1.9876e-01, -7.4032e-01,  2.3494e-02],\n",
       "        [ 4.6759e-01, -6.0317e-01,  3.0796e-01,  6.3031e-01, -1.3129e+00],\n",
       "        [-1.4697e-01, -1.6053e+00,  4.6007e-01, -1.3819e-01,  6.1302e-01],\n",
       "        [-8.6968e-02,  8.2363e-01,  1.5689e+00, -7.5890e-01, -5.7329e-01],\n",
       "        [ 5.5448e-01,  4.9221e-01, -1.9984e-02,  1.2856e+00, -6.8700e-01],\n",
       "        [-5.0862e-01,  1.7363e+00, -7.4993e-02, -1.5138e+00, -1.6022e-01],\n",
       "        [ 2.6567e-01,  2.3749e-03, -9.6876e-01, -1.2568e+00, -4.6934e-01],\n",
       "        [-2.4214e-01,  6.4046e-01, -4.7389e-01, -1.6566e+00, -4.2344e-01],\n",
       "        [ 1.3637e+00, -6.5284e-03,  8.3178e-01, -1.2724e+00, -1.4238e-01],\n",
       "        [-8.1988e-02,  1.9979e+00,  8.8980e-01, -2.5069e-01,  9.0310e-01],\n",
       "        [-1.5832e-01, -7.7591e-01,  1.3756e+00, -3.0197e+00,  1.4445e+00],\n",
       "        [ 2.0460e+00, -5.9102e-01, -1.9290e+00,  1.2833e+00, -3.2033e-02],\n",
       "        [-2.2499e+00, -1.2385e+00,  1.1165e+00, -6.7748e-01, -2.8059e-01],\n",
       "        [ 1.0942e+00,  3.7003e-01,  4.1162e-01, -1.1308e+00,  7.5565e-01],\n",
       "        [ 4.3888e-01,  7.1593e-01, -4.9391e-01, -2.5323e+00,  1.5953e-01],\n",
       "        [ 1.8021e+00, -5.3685e-02,  1.6552e-01, -1.0878e-01, -4.7902e-01],\n",
       "        [-1.3238e+00, -7.3092e-01,  1.2495e-01, -5.1265e-01, -2.1001e-02],\n",
       "        [ 8.1081e-01,  2.1571e+00, -9.0536e-01, -9.7797e-01,  1.0033e+00],\n",
       "        [ 1.5960e-01, -1.9311e+00,  9.3730e-01,  1.2225e+00,  6.9969e-01],\n",
       "        [ 7.3823e-01,  2.2812e-01, -1.2195e+00,  1.5090e+00,  1.0075e+00],\n",
       "        [-8.2614e-01, -1.1869e+00,  2.1936e-01, -8.1172e-01, -2.7830e-01],\n",
       "        [-1.1902e-01, -3.0696e-01,  2.3452e-01, -4.1017e-01, -1.2077e-03],\n",
       "        [ 3.3799e-01, -7.1143e-01, -2.9996e-02,  1.1469e+00, -3.4641e+00],\n",
       "        [-1.0377e+00, -1.0576e+00,  9.9926e-01,  6.6487e-01,  3.7016e-01],\n",
       "        [-1.1225e+00, -7.2117e-01, -5.8179e-01, -1.1653e+00,  9.6969e-01],\n",
       "        [ 1.0698e+00,  1.2511e+00,  1.5606e+00, -1.7587e+00,  9.2387e-01],\n",
       "        [ 4.6884e-01,  1.4788e+00, -1.1199e+00, -4.8899e-01, -3.3004e-01],\n",
       "        [ 5.8784e-02, -8.3484e-01,  9.7435e-01, -7.0232e-01,  1.7969e-01],\n",
       "        [-1.3162e+00,  6.9337e-01, -2.3976e-01, -6.4308e-01, -2.6167e+00],\n",
       "        [-1.6908e-01,  5.7425e-01,  1.1514e+00,  1.4980e+00,  7.6220e-01],\n",
       "        [-1.0870e-01,  9.8064e-02,  1.2876e+00,  7.0361e-01,  9.7530e-01],\n",
       "        [ 1.8097e-01, -7.9905e-01,  1.5082e+00,  6.2830e-01,  1.0360e+00],\n",
       "        [ 6.4196e-01, -1.4848e+00, -3.8311e-01, -4.9405e-01,  1.0312e+00],\n",
       "        [ 1.7791e+00,  6.5781e-01, -3.7001e-01, -2.3752e-01,  7.3138e-01],\n",
       "        [ 8.1269e-01, -2.2881e-01,  2.1662e-01,  6.0128e-01,  9.0422e-01],\n",
       "        [-2.5421e+00,  5.3441e-02,  3.7462e-02, -1.0307e+00,  8.9576e-01],\n",
       "        [-8.8123e-01,  1.1422e+00, -3.4259e-01,  1.4839e+00,  1.9674e+00],\n",
       "        [-1.1887e-01, -7.1068e-01,  9.2613e-01,  1.0187e+00, -1.0274e+00],\n",
       "        [ 1.1371e+00, -1.5404e-02,  2.2217e+00,  4.7553e-01,  4.0265e-01],\n",
       "        [-1.9547e-04, -2.7674e-02, -5.1529e-01,  7.0663e-02,  1.3232e+00],\n",
       "        [-1.4967e+00,  6.7566e-01, -1.4665e+00, -6.0045e-02, -1.5478e+00],\n",
       "        [-4.2019e-01, -5.6151e-01,  7.9029e-03,  2.4482e-01,  1.4311e-01],\n",
       "        [ 2.5649e-01,  7.4082e-02, -1.1038e+00,  6.2298e-01, -1.4735e+00],\n",
       "        [-5.7956e-01, -5.1109e-01, -3.9810e-01,  5.2610e-01, -6.0173e-01],\n",
       "        [-8.1252e-01,  9.5610e-01,  4.1863e-01, -1.0786e-01, -4.5301e-01],\n",
       "        [ 3.9820e-02, -4.9150e-01,  2.2728e-01, -2.0722e+00,  9.0503e-01],\n",
       "        [ 1.3834e+00, -7.9495e-01,  3.5242e-01, -3.1705e-01,  1.5981e+00],\n",
       "        [ 3.4150e-01, -7.4918e-01, -1.3994e-01,  2.8368e-01,  1.8487e+00],\n",
       "        [ 4.7278e-01,  8.6713e-01,  9.4905e-01,  1.4957e+00, -5.2610e-01],\n",
       "        [-7.4715e-01, -1.8114e-01,  7.5122e-01,  3.0588e-01, -7.2012e-01],\n",
       "        [-5.7675e-01, -7.6974e-01, -4.5740e-01, -3.4532e-01, -1.0068e-01],\n",
       "        [-1.1889e+00, -5.1689e-01,  1.7264e-01,  7.7513e-02, -3.2247e-01],\n",
       "        [ 1.8926e-01,  8.2962e-01,  2.2946e-01, -6.8168e-01, -1.3918e+00],\n",
       "        [-1.4143e-01,  7.5913e-02, -1.3531e+00, -3.8334e-01,  1.0837e+00],\n",
       "        [ 5.8596e-01,  8.6788e-01, -6.7888e-01,  1.0356e+00, -3.5321e-02],\n",
       "        [-6.4663e-01, -1.5173e+00,  4.7762e-02, -6.7670e-01, -3.8074e-01],\n",
       "        [-2.4082e-01,  1.1621e+00,  9.3939e-01,  1.5396e+00, -1.1659e+00],\n",
       "        [ 3.7724e-01, -1.1616e-01, -1.0658e+00, -5.9973e-02,  3.5084e-01],\n",
       "        [ 8.3390e-01,  2.0714e-01,  9.4343e-01,  9.1990e-01, -7.0812e-01],\n",
       "        [ 9.3557e-01, -1.1274e+00,  9.2009e-02,  1.1264e+00, -1.1937e+00],\n",
       "        [-1.2040e+00, -2.7830e-01,  2.7153e-01,  2.2591e-02, -5.2719e-01],\n",
       "        [-7.5565e-01,  3.8394e-01, -4.5701e-01,  9.5821e-02,  6.5855e-01],\n",
       "        [ 8.3232e-01,  4.4993e-01, -1.9792e+00, -7.8102e-02, -4.9347e-01]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(emb.state_dict()['weight'].shape)\n",
    "emb.state_dict()['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-9f626f8b00ed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "print(emb(x).shape)\n",
    "print(emb(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-c22d7028100b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0memb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "print(emb(x).view(1, -1).shape)\n",
    "emb(x).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.0682, -0.0824, -0.1613,  ...,  0.0075,  0.0211, -0.1730],\n",
      "        [ 0.1452,  0.1545, -0.1338,  ..., -0.0559,  0.1861, -0.1367],\n",
      "        [ 0.1486,  0.1155,  0.0959,  ...,  0.0907,  0.0113,  0.1604],\n",
      "        ...,\n",
      "        [ 0.1149,  0.0058, -0.0836,  ...,  0.0469, -0.1511, -0.1294],\n",
      "        [ 0.1305,  0.0394,  0.1792,  ...,  0.0004,  0.1719, -0.1286],\n",
      "        [ 0.0849, -0.0992,  0.1415,  ..., -0.0807, -0.0264, -0.0846]])), ('bias', tensor([ 1.8791e-02,  1.5262e-01,  1.5195e-01,  1.1731e-01,  9.1180e-02,\n",
      "         9.6480e-02,  4.2502e-02,  1.1213e-01, -9.4328e-02,  1.0605e-01,\n",
      "        -1.1435e-01,  5.4850e-02,  2.5943e-02,  1.8330e-01,  1.5268e-02,\n",
      "        -1.9103e-01,  3.5164e-02, -7.9650e-02, -9.0032e-02, -1.1632e-01,\n",
      "         7.7349e-02,  1.2977e-01, -1.3206e-01, -7.0709e-03, -1.3217e-01,\n",
      "         1.0222e-01,  1.4309e-01,  6.8925e-02,  1.8032e-01,  1.4187e-01,\n",
      "        -8.7168e-03, -1.8049e-01, -1.3915e-01, -1.3955e-01, -5.4416e-03,\n",
      "        -4.7703e-02,  1.4170e-02, -1.1165e-01,  8.0575e-02, -8.2097e-02,\n",
      "        -1.4046e-01,  3.2694e-02,  1.3629e-01, -3.4965e-02, -1.8000e-01,\n",
      "         1.9985e-01, -4.3542e-02, -1.2719e-01,  6.6760e-02, -7.9501e-02,\n",
      "         1.5509e-01, -1.8052e-01, -2.1848e-02, -3.9510e-02,  1.0491e-01,\n",
      "        -1.6003e-01,  2.8007e-02,  1.9504e-01,  6.2904e-02, -1.0821e-01,\n",
      "        -1.5135e-01,  8.8654e-02, -1.4844e-01,  1.1638e-01, -1.1350e-01,\n",
      "         3.4943e-02,  7.0526e-02, -1.9223e-01, -1.0413e-02,  1.9279e-01,\n",
      "        -8.3700e-02,  4.5477e-02, -7.7547e-02,  3.0370e-02,  1.5016e-02,\n",
      "        -1.0541e-01, -2.8198e-02, -3.6806e-02,  1.6726e-01, -8.6506e-03,\n",
      "        -4.4263e-02, -2.8169e-02, -6.9356e-02,  5.2480e-02, -1.6209e-01,\n",
      "         1.8009e-01,  1.7044e-01, -1.8149e-01,  5.3545e-02,  7.8581e-02,\n",
      "        -7.3880e-05,  1.0963e-01,  2.5457e-02,  5.8350e-02,  9.1533e-02,\n",
      "         1.0070e-01, -2.9289e-02, -1.6006e-01,  6.4635e-02,  1.4547e-01]))])\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 100\n",
    "lin1 = nn.Linear(len(x)*embd_size, hidden_size)\n",
    "print(lin1.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 25])\n",
      "tensor([[ 0.0682, -0.0824, -0.1613,  ...,  0.0075,  0.0211, -0.1730],\n",
      "        [ 0.1452,  0.1545, -0.1338,  ..., -0.0559,  0.1861, -0.1367],\n",
      "        [ 0.1486,  0.1155,  0.0959,  ...,  0.0907,  0.0113,  0.1604],\n",
      "        ...,\n",
      "        [ 0.1149,  0.0058, -0.0836,  ...,  0.0469, -0.1511, -0.1294],\n",
      "        [ 0.1305,  0.0394,  0.1792,  ...,  0.0004,  0.1719, -0.1286],\n",
      "        [ 0.0849, -0.0992,  0.1415,  ..., -0.0807, -0.0264, -0.0846]])\n"
     ]
    }
   ],
   "source": [
    "print(lin1.state_dict()['weight'].shape)\n",
    "print(lin1.state_dict()['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3971,  0.3754, -0.4184,  0.1931, -0.3988, -0.2276,  0.0472,  0.6021,\n",
       "          0.2035, -0.1339,  0.0322, -0.7428, -0.5011,  0.1104, -0.6841,  0.2021,\n",
       "          0.4721,  0.0744, -0.0591, -0.3688,  0.5280, -0.2357, -0.1436, -0.6866,\n",
       "          0.0530,  0.2143,  0.9329,  0.1224, -0.1782,  0.0445, -0.2867,  0.4525,\n",
       "         -0.4842,  0.2364,  1.3188, -0.2338, -0.2273, -0.7911,  0.8791, -0.1656,\n",
       "          0.8919, -0.4287, -0.7792, -0.0292, -0.0628,  0.3437,  0.6424,  0.0903,\n",
       "          0.3973, -0.1186,  0.6307, -0.7290,  0.1930, -0.1160,  0.3763,  0.3032,\n",
       "         -0.0249, -0.4606, -0.2629, -0.3670,  0.2756,  0.3389,  0.7264, -0.2258,\n",
       "          0.5897, -0.0961,  0.1126, -0.8335, -0.5276,  0.0637, -0.4331,  1.4119,\n",
       "         -0.0479, -0.8726, -0.0240,  0.2704,  0.0539, -0.2571,  0.0348,  0.2081,\n",
       "          0.1643,  0.6453,  0.6159, -0.3658,  0.2794, -0.6706,  0.2810, -0.5973,\n",
       "          0.1143, -0.3701,  0.0720,  0.5984, -0.2203, -0.3437, -0.8729, -0.9318,\n",
       "         -0.2127, -0.2728, -0.4762,  0.5720]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lin1(emb(x).view(1, -1)).shape)\n",
    "lin1(emb(x).view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3971, 0.3754, 0.0000, 0.1931, 0.0000, 0.0000, 0.0472, 0.6021, 0.2035,\n",
       "         0.0000, 0.0322, 0.0000, 0.0000, 0.1104, 0.0000, 0.2021, 0.4721, 0.0744,\n",
       "         0.0000, 0.0000, 0.5280, 0.0000, 0.0000, 0.0000, 0.0530, 0.2143, 0.9329,\n",
       "         0.1224, 0.0000, 0.0445, 0.0000, 0.4525, 0.0000, 0.2364, 1.3188, 0.0000,\n",
       "         0.0000, 0.0000, 0.8791, 0.0000, 0.8919, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.3437, 0.6424, 0.0903, 0.3973, 0.0000, 0.6307, 0.0000, 0.1930, 0.0000,\n",
       "         0.3763, 0.3032, 0.0000, 0.0000, 0.0000, 0.0000, 0.2756, 0.3389, 0.7264,\n",
       "         0.0000, 0.5897, 0.0000, 0.1126, 0.0000, 0.0000, 0.0637, 0.0000, 1.4119,\n",
       "         0.0000, 0.0000, 0.0000, 0.2704, 0.0539, 0.0000, 0.0348, 0.2081, 0.1643,\n",
       "         0.6453, 0.6159, 0.0000, 0.2794, 0.0000, 0.2810, 0.0000, 0.1143, 0.0000,\n",
       "         0.0720, 0.5984, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.5720]], grad_fn=<ThresholdBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = nn.ReLU()\n",
    "print(relu(lin1(emb(x).view(1, -1))).shape)\n",
    "relu(lin1(emb(x).view(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1388, 100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0163, -0.0994,  0.0672,  ...,  0.0186, -0.0159,  0.0627],\n",
       "        [-0.0074, -0.0539, -0.0873,  ..., -0.0316, -0.0493,  0.0676],\n",
       "        [-0.0643, -0.0600, -0.0529,  ..., -0.0705, -0.0151, -0.0443],\n",
       "        ...,\n",
       "        [-0.0546, -0.0097, -0.0718,  ..., -0.0164,  0.0428,  0.0612],\n",
       "        [-0.0227, -0.0751, -0.0962,  ...,  0.0180,  0.0922,  0.0302],\n",
       "        [-0.0727, -0.0318, -0.0375,  ...,  0.0241, -0.0968,  0.0940]])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin2 = nn.Linear(hidden_size, len(w2v_dataset.vocab))\n",
    "print(lin2.state_dict()['weight'].shape)\n",
    "lin2.state_dict()['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1388])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2554,  0.0995,  0.1225,  ..., -0.0309, -0.2753, -0.0698]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_x = relu(lin1(emb(x).view(1, -1)))\n",
    "print(lin2(h_x).shape)\n",
    "lin2(h_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-7.008087635040283,\n",
       "  -7.164051532745361,\n",
       "  -7.140991687774658,\n",
       "  -6.906752109527588,\n",
       "  -7.240259647369385,\n",
       "  -7.038198471069336,\n",
       "  -7.3443193435668945,\n",
       "  -6.702448844909668,\n",
       "  -7.318045616149902,\n",
       "  -6.870830535888672,\n",
       "  -7.056977272033691,\n",
       "  -7.279659271240234,\n",
       "  -7.4093427658081055,\n",
       "  -6.968787670135498,\n",
       "  -7.378303527832031,\n",
       "  -7.249201774597168,\n",
       "  -7.308876037597656,\n",
       "  -7.218778610229492,\n",
       "  -7.1557393074035645,\n",
       "  -7.171926975250244,\n",
       "  -7.515322208404541,\n",
       "  -7.045439720153809,\n",
       "  -7.370819091796875,\n",
       "  -7.049893379211426,\n",
       "  -6.97169828414917,\n",
       "  -7.340801239013672,\n",
       "  -7.147621154785156,\n",
       "  -6.899478912353516,\n",
       "  -7.073168754577637,\n",
       "  -6.781639099121094,\n",
       "  -7.03122615814209,\n",
       "  -7.764267444610596,\n",
       "  -7.429055213928223,\n",
       "  -7.157585144042969,\n",
       "  -7.229382038116455,\n",
       "  -7.688353061676025,\n",
       "  -7.3556647300720215,\n",
       "  -7.453991413116455,\n",
       "  -7.008212566375732,\n",
       "  -7.171889305114746,\n",
       "  -7.36814546585083,\n",
       "  -7.133216857910156,\n",
       "  -7.111328125,\n",
       "  -7.182351112365723,\n",
       "  -7.2988715171813965,\n",
       "  -7.473907947540283,\n",
       "  -7.3404011726379395,\n",
       "  -6.776766777038574,\n",
       "  -7.187447547912598,\n",
       "  -7.256719589233398,\n",
       "  -6.980386257171631,\n",
       "  -7.2915191650390625,\n",
       "  -7.440740585327148,\n",
       "  -7.723786354064941,\n",
       "  -7.576779842376709,\n",
       "  -7.251850605010986,\n",
       "  -6.886423587799072,\n",
       "  -7.5162434577941895,\n",
       "  -7.305513858795166,\n",
       "  -7.59526252746582,\n",
       "  -7.2951483726501465,\n",
       "  -7.429166316986084,\n",
       "  -7.5000433921813965,\n",
       "  -7.3382720947265625,\n",
       "  -7.180732727050781,\n",
       "  -7.078279972076416,\n",
       "  -7.16770601272583,\n",
       "  -7.546836853027344,\n",
       "  -7.060425281524658,\n",
       "  -7.47640323638916,\n",
       "  -6.997842311859131,\n",
       "  -7.338111877441406,\n",
       "  -7.239778995513916,\n",
       "  -7.370309829711914,\n",
       "  -7.161778450012207,\n",
       "  -7.3959856033325195,\n",
       "  -7.501739025115967,\n",
       "  -7.201857089996338,\n",
       "  -7.073707103729248,\n",
       "  -7.169435977935791,\n",
       "  -7.180130481719971,\n",
       "  -7.846404552459717,\n",
       "  -7.058173656463623,\n",
       "  -7.240304470062256,\n",
       "  -7.25154972076416,\n",
       "  -7.102787494659424,\n",
       "  -7.902781963348389,\n",
       "  -6.835278034210205,\n",
       "  -7.035390853881836,\n",
       "  -7.457019329071045,\n",
       "  -7.223503589630127,\n",
       "  -7.229985237121582,\n",
       "  -7.258909225463867,\n",
       "  -7.216009616851807,\n",
       "  -7.207745552062988,\n",
       "  -7.826532363891602,\n",
       "  -7.230821132659912,\n",
       "  -7.2268476486206055,\n",
       "  -7.286680698394775,\n",
       "  -7.344486236572266,\n",
       "  -7.44877815246582,\n",
       "  -7.567476272583008,\n",
       "  -7.315956115722656,\n",
       "  -7.135549068450928,\n",
       "  -7.0199666023254395,\n",
       "  -7.292378902435303,\n",
       "  -7.1722636222839355,\n",
       "  -7.245265960693359,\n",
       "  -7.233574390411377,\n",
       "  -7.476742267608643,\n",
       "  -7.250790119171143,\n",
       "  -7.08611536026001,\n",
       "  -7.3299360275268555,\n",
       "  -7.068906784057617,\n",
       "  -7.279549598693848,\n",
       "  -7.094468593597412,\n",
       "  -7.340001583099365,\n",
       "  -7.418186187744141,\n",
       "  -7.175532341003418,\n",
       "  -7.308940887451172,\n",
       "  -7.1732048988342285,\n",
       "  -7.142141819000244,\n",
       "  -7.537625789642334,\n",
       "  -7.209445953369141,\n",
       "  -7.136079788208008,\n",
       "  -7.190145492553711,\n",
       "  -7.40188455581665,\n",
       "  -6.883933067321777,\n",
       "  -7.356791019439697,\n",
       "  -7.445075988769531,\n",
       "  -7.630074977874756,\n",
       "  -7.320050239562988,\n",
       "  -7.271612167358398,\n",
       "  -7.624539375305176,\n",
       "  -7.508281707763672,\n",
       "  -7.15703010559082,\n",
       "  -7.454744815826416,\n",
       "  -7.085598945617676,\n",
       "  -7.308098793029785,\n",
       "  -7.266774654388428,\n",
       "  -7.111052513122559,\n",
       "  -6.866523265838623,\n",
       "  -7.220690727233887,\n",
       "  -7.063572883605957,\n",
       "  -7.065986156463623,\n",
       "  -6.986056804656982,\n",
       "  -7.0524420738220215,\n",
       "  -7.208313465118408,\n",
       "  -7.102643013000488,\n",
       "  -7.219526290893555,\n",
       "  -7.367524147033691,\n",
       "  -7.093322277069092,\n",
       "  -7.144153594970703,\n",
       "  -7.310028076171875,\n",
       "  -6.9398932456970215,\n",
       "  -7.048643589019775,\n",
       "  -7.587108135223389,\n",
       "  -7.291459560394287,\n",
       "  -7.080615043640137,\n",
       "  -7.398488521575928,\n",
       "  -7.543788433074951,\n",
       "  -7.312781810760498,\n",
       "  -7.2965898513793945,\n",
       "  -7.206351280212402,\n",
       "  -7.347085952758789,\n",
       "  -7.036767959594727,\n",
       "  -7.564351558685303,\n",
       "  -7.619997978210449,\n",
       "  -7.232138156890869,\n",
       "  -7.245752811431885,\n",
       "  -7.270204544067383,\n",
       "  -7.206104755401611,\n",
       "  -7.314797878265381,\n",
       "  -7.361177921295166,\n",
       "  -7.184865951538086,\n",
       "  -6.882903575897217,\n",
       "  -7.224924564361572,\n",
       "  -7.388716697692871,\n",
       "  -6.981986045837402,\n",
       "  -7.3132643699646,\n",
       "  -7.423131942749023,\n",
       "  -7.459348201751709,\n",
       "  -7.684935092926025,\n",
       "  -7.31715726852417,\n",
       "  -7.210917949676514,\n",
       "  -7.1970133781433105,\n",
       "  -7.613353252410889,\n",
       "  -7.4663214683532715,\n",
       "  -7.46829080581665,\n",
       "  -7.190290927886963,\n",
       "  -6.908935546875,\n",
       "  -7.3245158195495605,\n",
       "  -7.589381694793701,\n",
       "  -7.441150665283203,\n",
       "  -7.278570652008057,\n",
       "  -7.2422380447387695,\n",
       "  -7.104700565338135,\n",
       "  -7.448566436767578,\n",
       "  -7.164538383483887,\n",
       "  -7.085138320922852,\n",
       "  -7.331622123718262,\n",
       "  -7.310694217681885,\n",
       "  -7.044249057769775,\n",
       "  -7.103610038757324,\n",
       "  -7.032536029815674,\n",
       "  -7.335468769073486,\n",
       "  -6.825035095214844,\n",
       "  -7.12945556640625,\n",
       "  -7.435810565948486,\n",
       "  -7.354545593261719,\n",
       "  -7.188465118408203,\n",
       "  -7.020740985870361,\n",
       "  -7.118144989013672,\n",
       "  -7.216371536254883,\n",
       "  -7.488961219787598,\n",
       "  -7.5015869140625,\n",
       "  -7.421886920928955,\n",
       "  -7.11024808883667,\n",
       "  -7.183154582977295,\n",
       "  -7.218673229217529,\n",
       "  -7.120098114013672,\n",
       "  -7.62611198425293,\n",
       "  -7.2409796714782715,\n",
       "  -7.137780666351318,\n",
       "  -7.324314117431641,\n",
       "  -7.386785507202148,\n",
       "  -7.278824329376221,\n",
       "  -7.361059188842773,\n",
       "  -7.259631633758545,\n",
       "  -7.174863338470459,\n",
       "  -7.115939140319824,\n",
       "  -7.361763954162598,\n",
       "  -7.3205461502075195,\n",
       "  -7.228708267211914,\n",
       "  -7.267870903015137,\n",
       "  -7.376798629760742,\n",
       "  -7.27490758895874,\n",
       "  -7.51826810836792,\n",
       "  -6.901567459106445,\n",
       "  -7.195174694061279,\n",
       "  -7.542396545410156,\n",
       "  -7.310030937194824,\n",
       "  -6.962653636932373,\n",
       "  -7.231578826904297,\n",
       "  -7.149742126464844,\n",
       "  -7.346371173858643,\n",
       "  -7.341416835784912,\n",
       "  -7.517022132873535,\n",
       "  -7.091804504394531,\n",
       "  -7.641845226287842,\n",
       "  -7.070198059082031,\n",
       "  -6.956513404846191,\n",
       "  -7.0185866355896,\n",
       "  -7.370710849761963,\n",
       "  -7.650082111358643,\n",
       "  -7.091221809387207,\n",
       "  -7.273286819458008,\n",
       "  -7.045952796936035,\n",
       "  -6.999576091766357,\n",
       "  -7.1183762550354,\n",
       "  -7.0323944091796875,\n",
       "  -7.578618049621582,\n",
       "  -7.5854411125183105,\n",
       "  -7.338098049163818,\n",
       "  -7.104933261871338,\n",
       "  -6.989391326904297,\n",
       "  -7.336292743682861,\n",
       "  -6.946887969970703,\n",
       "  -7.400816917419434,\n",
       "  -7.127501964569092,\n",
       "  -7.412625789642334,\n",
       "  -7.264148712158203,\n",
       "  -7.389139652252197,\n",
       "  -7.399169921875,\n",
       "  -7.060143947601318,\n",
       "  -7.212252616882324,\n",
       "  -7.342526912689209,\n",
       "  -6.880123615264893,\n",
       "  -7.121822357177734,\n",
       "  -7.251370429992676,\n",
       "  -7.310711860656738,\n",
       "  -7.215672969818115,\n",
       "  -7.2161545753479,\n",
       "  -7.725754261016846,\n",
       "  -7.435445785522461,\n",
       "  -7.281432628631592,\n",
       "  -7.215698719024658,\n",
       "  -7.226632118225098,\n",
       "  -7.168694972991943,\n",
       "  -7.340027809143066,\n",
       "  -7.31149435043335,\n",
       "  -7.118497371673584,\n",
       "  -7.012585639953613,\n",
       "  -7.694498538970947,\n",
       "  -7.188324928283691,\n",
       "  -7.097322463989258,\n",
       "  -7.2533416748046875,\n",
       "  -7.139651298522949,\n",
       "  -6.991909503936768,\n",
       "  -7.401878833770752,\n",
       "  -7.5344390869140625,\n",
       "  -6.972663402557373,\n",
       "  -7.2684173583984375,\n",
       "  -7.335579872131348,\n",
       "  -7.282690048217773,\n",
       "  -7.1993865966796875,\n",
       "  -7.194159507751465,\n",
       "  -7.428945064544678,\n",
       "  -7.084896564483643,\n",
       "  -7.193120956420898,\n",
       "  -7.292202472686768,\n",
       "  -6.886526584625244,\n",
       "  -7.414813041687012,\n",
       "  -7.121539115905762,\n",
       "  -7.1893415451049805,\n",
       "  -7.068497657775879,\n",
       "  -7.15379524230957,\n",
       "  -7.049465656280518,\n",
       "  -7.281877517700195,\n",
       "  -7.18889856338501,\n",
       "  -7.569902420043945,\n",
       "  -7.080122947692871,\n",
       "  -6.974414825439453,\n",
       "  -7.158703804016113,\n",
       "  -7.140672206878662,\n",
       "  -7.009382724761963,\n",
       "  -7.285759449005127,\n",
       "  -7.019897937774658,\n",
       "  -7.321495532989502,\n",
       "  -7.0865631103515625,\n",
       "  -7.292575359344482,\n",
       "  -7.312102317810059,\n",
       "  -7.015326976776123,\n",
       "  -7.126448631286621,\n",
       "  -7.338527202606201,\n",
       "  -7.146791458129883,\n",
       "  -7.143979549407959,\n",
       "  -7.3734846115112305,\n",
       "  -7.2211737632751465,\n",
       "  -7.369081497192383,\n",
       "  -7.271825313568115,\n",
       "  -7.1077141761779785,\n",
       "  -7.2994866371154785,\n",
       "  -7.325229167938232,\n",
       "  -7.289285659790039,\n",
       "  -7.577617645263672,\n",
       "  -7.0761213302612305,\n",
       "  -7.534237384796143,\n",
       "  -7.318081378936768,\n",
       "  -7.387694358825684,\n",
       "  -7.56229305267334,\n",
       "  -7.31202507019043,\n",
       "  -7.524993419647217,\n",
       "  -7.379288196563721,\n",
       "  -7.212660789489746,\n",
       "  -7.64198637008667,\n",
       "  -6.797666072845459,\n",
       "  -7.5824713706970215,\n",
       "  -7.5662946701049805,\n",
       "  -7.265326023101807,\n",
       "  -7.457360744476318,\n",
       "  -7.160262107849121,\n",
       "  -7.3824381828308105,\n",
       "  -7.274837970733643,\n",
       "  -7.310189723968506,\n",
       "  -7.067305564880371,\n",
       "  -7.072739124298096,\n",
       "  -7.087245941162109,\n",
       "  -7.2341084480285645,\n",
       "  -7.179457187652588,\n",
       "  -6.69849967956543,\n",
       "  -7.206754207611084,\n",
       "  -7.20123815536499,\n",
       "  -7.440639495849609,\n",
       "  -7.554030418395996,\n",
       "  -7.513939380645752,\n",
       "  -7.543179988861084,\n",
       "  -7.64046573638916,\n",
       "  -6.927505016326904,\n",
       "  -7.39849853515625,\n",
       "  -7.656201362609863,\n",
       "  -6.987860679626465,\n",
       "  -7.052278518676758,\n",
       "  -6.902440547943115,\n",
       "  -7.132090091705322,\n",
       "  -7.212708950042725,\n",
       "  -7.450136661529541,\n",
       "  -6.957788467407227,\n",
       "  -7.689050197601318,\n",
       "  -7.442686557769775,\n",
       "  -7.109559535980225,\n",
       "  -7.28574800491333,\n",
       "  -7.173458099365234,\n",
       "  -7.261765480041504,\n",
       "  -7.358128547668457,\n",
       "  -7.533226013183594,\n",
       "  -7.585484504699707,\n",
       "  -7.0590996742248535,\n",
       "  -7.2987775802612305,\n",
       "  -7.091790199279785,\n",
       "  -7.15698766708374,\n",
       "  -7.125953674316406,\n",
       "  -6.888054847717285,\n",
       "  -7.119128227233887,\n",
       "  -7.342703819274902,\n",
       "  -7.295034408569336,\n",
       "  -7.431206703186035,\n",
       "  -7.315360069274902,\n",
       "  -7.377617835998535,\n",
       "  -7.395567417144775,\n",
       "  -7.30953311920166,\n",
       "  -7.3681640625,\n",
       "  -7.010122299194336,\n",
       "  -7.356379508972168,\n",
       "  -6.872463226318359,\n",
       "  -6.94442081451416,\n",
       "  -7.064408302307129,\n",
       "  -7.503993988037109,\n",
       "  -7.182483673095703,\n",
       "  -7.065713405609131,\n",
       "  -7.349286079406738,\n",
       "  -7.102651119232178,\n",
       "  -7.045037746429443,\n",
       "  -7.135784149169922,\n",
       "  -7.274330139160156,\n",
       "  -6.785159587860107,\n",
       "  -7.568383693695068,\n",
       "  -7.181751728057861,\n",
       "  -7.236179351806641,\n",
       "  -7.346180438995361,\n",
       "  -7.5546770095825195,\n",
       "  -7.234004020690918,\n",
       "  -7.429477214813232,\n",
       "  -7.198612689971924,\n",
       "  -7.851400852203369,\n",
       "  -7.082934379577637,\n",
       "  -7.663540363311768,\n",
       "  -7.332881450653076,\n",
       "  -7.168850421905518,\n",
       "  -7.378788948059082,\n",
       "  -7.512359619140625,\n",
       "  -7.535367012023926,\n",
       "  -7.432846546173096,\n",
       "  -7.225471019744873,\n",
       "  -7.63249397277832,\n",
       "  -7.182903289794922,\n",
       "  -7.284243106842041,\n",
       "  -7.1780104637146,\n",
       "  -7.022204399108887,\n",
       "  -7.222253799438477,\n",
       "  -6.994656562805176,\n",
       "  -7.153406620025635,\n",
       "  -7.188506603240967,\n",
       "  -7.131659030914307,\n",
       "  -7.004475116729736,\n",
       "  -7.507574081420898,\n",
       "  -7.320845127105713,\n",
       "  -7.426556587219238,\n",
       "  -7.168865203857422,\n",
       "  -7.21493673324585,\n",
       "  -7.562675952911377,\n",
       "  -6.928420543670654,\n",
       "  -7.431334018707275,\n",
       "  -7.254951000213623,\n",
       "  -7.322110176086426,\n",
       "  -7.303622722625732,\n",
       "  -6.923159122467041,\n",
       "  -7.187567710876465,\n",
       "  -7.239781856536865,\n",
       "  -7.308712482452393,\n",
       "  -7.285854339599609,\n",
       "  -7.341442584991455,\n",
       "  -7.53127384185791,\n",
       "  -7.067058086395264,\n",
       "  -7.3571014404296875,\n",
       "  -7.1717610359191895,\n",
       "  -7.502933502197266,\n",
       "  -7.148255348205566,\n",
       "  -7.194136619567871,\n",
       "  -7.223097324371338,\n",
       "  -7.422771453857422,\n",
       "  -7.352560520172119,\n",
       "  -6.978544235229492,\n",
       "  -6.986959934234619,\n",
       "  -7.384133338928223,\n",
       "  -7.126054286956787,\n",
       "  -7.227176666259766,\n",
       "  -7.502737522125244,\n",
       "  -6.896965503692627,\n",
       "  -7.509948253631592,\n",
       "  -7.3582940101623535,\n",
       "  -6.907540321350098,\n",
       "  -7.321690559387207,\n",
       "  -6.870067119598389,\n",
       "  -6.928683280944824,\n",
       "  -7.2920823097229,\n",
       "  -7.066981315612793,\n",
       "  -7.470550537109375,\n",
       "  -7.163966178894043,\n",
       "  -7.245852947235107,\n",
       "  -7.25157356262207,\n",
       "  -7.487972736358643,\n",
       "  -7.013387203216553,\n",
       "  -7.548333644866943,\n",
       "  -7.09999418258667,\n",
       "  -7.593997478485107,\n",
       "  -7.099153995513916,\n",
       "  -6.939497947692871,\n",
       "  -7.09487247467041,\n",
       "  -7.211186408996582,\n",
       "  -7.107476711273193,\n",
       "  -7.098190784454346,\n",
       "  -6.760207176208496,\n",
       "  -7.484528064727783,\n",
       "  -7.090427398681641,\n",
       "  -7.288841724395752,\n",
       "  -6.994300365447998,\n",
       "  -7.054564952850342,\n",
       "  -7.150937080383301,\n",
       "  -7.141282081604004,\n",
       "  -7.250358581542969,\n",
       "  -7.468142032623291,\n",
       "  -7.04219388961792,\n",
       "  -7.09532356262207,\n",
       "  -7.568829536437988,\n",
       "  -6.747628688812256,\n",
       "  -7.3507771492004395,\n",
       "  -7.230055332183838,\n",
       "  -7.066439151763916,\n",
       "  -7.518764495849609,\n",
       "  -7.254426002502441,\n",
       "  -7.138576984405518,\n",
       "  -7.3989033699035645,\n",
       "  -6.7848076820373535,\n",
       "  -7.003071308135986,\n",
       "  -7.528708457946777,\n",
       "  -7.087723255157471,\n",
       "  -7.135274887084961,\n",
       "  -7.206984043121338,\n",
       "  -6.916204452514648,\n",
       "  -6.832090377807617,\n",
       "  -7.344672203063965,\n",
       "  -7.127810478210449,\n",
       "  -7.258560657501221,\n",
       "  -6.938920497894287,\n",
       "  -6.876552104949951,\n",
       "  -7.368716239929199,\n",
       "  -7.563944339752197,\n",
       "  -7.312227249145508,\n",
       "  -7.5064377784729,\n",
       "  -7.324972629547119,\n",
       "  -7.246589660644531,\n",
       "  -7.3489670753479,\n",
       "  -7.391075134277344,\n",
       "  -7.160021781921387,\n",
       "  -7.063024044036865,\n",
       "  -7.342509746551514,\n",
       "  -7.350038051605225,\n",
       "  -7.056115627288818,\n",
       "  -7.132362365722656,\n",
       "  -7.0837907791137695,\n",
       "  -7.389150142669678,\n",
       "  -7.179910659790039,\n",
       "  -7.264247894287109,\n",
       "  -7.650059223175049,\n",
       "  -7.359536647796631,\n",
       "  -7.325995922088623,\n",
       "  -7.366162300109863,\n",
       "  -7.593106269836426,\n",
       "  -7.04752254486084,\n",
       "  -6.9291090965271,\n",
       "  -6.929476261138916,\n",
       "  -7.279200553894043,\n",
       "  -7.410257816314697,\n",
       "  -7.37819766998291,\n",
       "  -7.382796287536621,\n",
       "  -7.204224109649658,\n",
       "  -7.470205307006836,\n",
       "  -7.122041702270508,\n",
       "  -7.737987518310547,\n",
       "  -7.003488063812256,\n",
       "  -7.456414699554443,\n",
       "  -7.3453168869018555,\n",
       "  -7.603869438171387,\n",
       "  -7.023723125457764,\n",
       "  -7.196075439453125,\n",
       "  -7.3560614585876465,\n",
       "  -7.268832683563232,\n",
       "  -7.199492454528809,\n",
       "  -7.129806995391846,\n",
       "  -7.181900501251221,\n",
       "  -7.553534030914307,\n",
       "  -7.351804256439209,\n",
       "  -7.308698654174805,\n",
       "  -7.383578777313232,\n",
       "  -7.019040584564209,\n",
       "  -7.035858154296875,\n",
       "  -7.682042598724365,\n",
       "  -7.457505702972412,\n",
       "  -7.3979597091674805,\n",
       "  -7.485100746154785,\n",
       "  -7.200932502746582,\n",
       "  -7.2758612632751465,\n",
       "  -7.0597405433654785,\n",
       "  -7.088374614715576,\n",
       "  -7.395416259765625,\n",
       "  -7.090122222900391,\n",
       "  -7.284698009490967,\n",
       "  -6.991803169250488,\n",
       "  -6.978333473205566,\n",
       "  -7.245726585388184,\n",
       "  -7.226889133453369,\n",
       "  -7.706818580627441,\n",
       "  -7.3304338455200195,\n",
       "  -7.249930381774902,\n",
       "  -7.4382405281066895,\n",
       "  -7.141624927520752,\n",
       "  -7.575928688049316,\n",
       "  -7.6355438232421875,\n",
       "  -7.449550151824951,\n",
       "  -7.290146827697754,\n",
       "  -6.64711332321167,\n",
       "  -6.855515480041504,\n",
       "  -7.705402851104736,\n",
       "  -7.114187240600586,\n",
       "  -7.304945468902588,\n",
       "  -7.260470390319824,\n",
       "  -7.179008960723877,\n",
       "  -7.294332027435303,\n",
       "  -7.380050182342529,\n",
       "  -7.026981830596924,\n",
       "  -7.230626583099365,\n",
       "  -7.030556678771973,\n",
       "  -7.158227920532227,\n",
       "  -7.31289529800415,\n",
       "  -6.84520959854126,\n",
       "  -7.351581573486328,\n",
       "  -7.083564281463623,\n",
       "  -7.019412994384766,\n",
       "  -7.30252742767334,\n",
       "  -7.451486587524414,\n",
       "  -7.09081506729126,\n",
       "  -7.362727642059326,\n",
       "  -7.322555065155029,\n",
       "  -7.503492832183838,\n",
       "  -6.935218334197998,\n",
       "  -7.5884294509887695,\n",
       "  -7.11593770980835,\n",
       "  -7.348957538604736,\n",
       "  -7.063962936401367,\n",
       "  -6.9889984130859375,\n",
       "  -6.960238456726074,\n",
       "  -7.273501396179199,\n",
       "  -7.404116153717041,\n",
       "  -7.235684871673584,\n",
       "  -7.288626670837402,\n",
       "  -7.028456211090088,\n",
       "  -7.273512363433838,\n",
       "  -7.306680679321289,\n",
       "  -7.5818562507629395,\n",
       "  -7.190190315246582,\n",
       "  -6.947789192199707,\n",
       "  -7.145994186401367,\n",
       "  -7.524310111999512,\n",
       "  -7.105079174041748,\n",
       "  -7.2363691329956055,\n",
       "  -7.126681804656982,\n",
       "  -7.604705333709717,\n",
       "  -7.0085601806640625,\n",
       "  -7.3340911865234375,\n",
       "  -7.353387832641602,\n",
       "  -7.093203067779541,\n",
       "  -7.097188949584961,\n",
       "  -7.480080604553223,\n",
       "  -7.17892599105835,\n",
       "  -7.359190940856934,\n",
       "  -7.589475154876709,\n",
       "  -7.264500141143799,\n",
       "  -7.437931537628174,\n",
       "  -6.893824577331543,\n",
       "  -7.128139495849609,\n",
       "  -7.214230537414551,\n",
       "  -7.609269142150879,\n",
       "  -7.406997203826904,\n",
       "  -7.075199127197266,\n",
       "  -7.259389400482178,\n",
       "  -7.198231220245361,\n",
       "  -7.2975568771362305,\n",
       "  -7.493800163269043,\n",
       "  -7.467618942260742,\n",
       "  -7.341710567474365,\n",
       "  -7.5178327560424805,\n",
       "  -6.976593494415283,\n",
       "  -7.637584209442139,\n",
       "  -7.226366996765137,\n",
       "  -7.351396083831787,\n",
       "  -7.166980743408203,\n",
       "  -7.216150283813477,\n",
       "  -7.4260334968566895,\n",
       "  -7.049006462097168,\n",
       "  -7.362462043762207,\n",
       "  -7.510459899902344,\n",
       "  -7.19711971282959,\n",
       "  -7.263354778289795,\n",
       "  -7.270376682281494,\n",
       "  -7.768497467041016,\n",
       "  -7.294398307800293,\n",
       "  -7.3398118019104,\n",
       "  -7.657429218292236,\n",
       "  -7.150038719177246,\n",
       "  -7.20359992980957,\n",
       "  -7.472484588623047,\n",
       "  -7.267033576965332,\n",
       "  -6.849665641784668,\n",
       "  -7.287302494049072,\n",
       "  -7.007063865661621,\n",
       "  -7.462552070617676,\n",
       "  -6.539402961730957,\n",
       "  -7.422268390655518,\n",
       "  -7.052239894866943,\n",
       "  -7.5597991943359375,\n",
       "  -7.397984027862549,\n",
       "  -6.969964027404785,\n",
       "  -7.024586200714111,\n",
       "  -7.043013572692871,\n",
       "  -7.495115280151367,\n",
       "  -7.375790596008301,\n",
       "  -7.222197532653809,\n",
       "  -7.0980143547058105,\n",
       "  -7.292623519897461,\n",
       "  -7.4081854820251465,\n",
       "  -7.403485298156738,\n",
       "  -7.145370006561279,\n",
       "  -7.472646713256836,\n",
       "  -7.261841297149658,\n",
       "  -7.3918681144714355,\n",
       "  -7.260292053222656,\n",
       "  -6.964537620544434,\n",
       "  -7.308478355407715,\n",
       "  -7.093349933624268,\n",
       "  -7.741617679595947,\n",
       "  -7.025124549865723,\n",
       "  -7.508439540863037,\n",
       "  -7.155754089355469,\n",
       "  -7.081514835357666,\n",
       "  -7.253499984741211,\n",
       "  -7.014280319213867,\n",
       "  -7.3844170570373535,\n",
       "  -7.4022417068481445,\n",
       "  -6.906058311462402,\n",
       "  -7.030508995056152,\n",
       "  -7.093587398529053,\n",
       "  -7.148382186889648,\n",
       "  -7.099871635437012,\n",
       "  -7.410500526428223,\n",
       "  -7.520951747894287,\n",
       "  -7.28519344329834,\n",
       "  -7.1607136726379395,\n",
       "  -7.446990489959717,\n",
       "  -7.342316150665283,\n",
       "  -6.915114402770996,\n",
       "  -7.1703619956970215,\n",
       "  -7.621894836425781,\n",
       "  -7.2928266525268555,\n",
       "  -7.1110148429870605,\n",
       "  -7.743503093719482,\n",
       "  -7.518491268157959,\n",
       "  -7.4123711585998535,\n",
       "  -7.351827144622803,\n",
       "  -7.477572917938232,\n",
       "  -7.634220123291016,\n",
       "  -7.185583591461182,\n",
       "  -7.146387577056885,\n",
       "  -7.5467729568481445,\n",
       "  -7.375387191772461,\n",
       "  -7.247828006744385,\n",
       "  -6.97022819519043,\n",
       "  -7.230842590332031,\n",
       "  -7.443815231323242,\n",
       "  -7.217484951019287,\n",
       "  -7.241519927978516,\n",
       "  -7.119336128234863,\n",
       "  -7.513772010803223,\n",
       "  -7.563791751861572,\n",
       "  -7.149266719818115,\n",
       "  -7.537530899047852,\n",
       "  -7.388221263885498,\n",
       "  -7.540180206298828,\n",
       "  -7.190778732299805,\n",
       "  -7.3592915534973145,\n",
       "  -7.370617389678955,\n",
       "  -7.361199378967285,\n",
       "  -6.9677581787109375,\n",
       "  -7.391101837158203,\n",
       "  -7.532656669616699,\n",
       "  -7.037534236907959,\n",
       "  -7.106225967407227,\n",
       "  -7.777737140655518,\n",
       "  -7.4260406494140625,\n",
       "  -7.085557460784912,\n",
       "  -7.011632919311523,\n",
       "  -7.043148517608643,\n",
       "  -7.497377872467041,\n",
       "  -6.8293256759643555,\n",
       "  -7.398390769958496,\n",
       "  -7.108123302459717,\n",
       "  -7.287014961242676,\n",
       "  -7.390501022338867,\n",
       "  -7.232253551483154,\n",
       "  -7.427765369415283,\n",
       "  -7.081329345703125,\n",
       "  -6.9941086769104,\n",
       "  -7.521411418914795,\n",
       "  -7.553830146789551,\n",
       "  -7.031466007232666,\n",
       "  -7.345997333526611,\n",
       "  -7.162090301513672,\n",
       "  -7.3104095458984375,\n",
       "  -7.079995632171631,\n",
       "  -7.291949272155762,\n",
       "  -7.259164810180664,\n",
       "  -7.3105788230896,\n",
       "  -7.204899787902832,\n",
       "  -7.254565715789795,\n",
       "  -7.398976802825928,\n",
       "  -7.331415176391602,\n",
       "  -7.409006118774414,\n",
       "  -6.77371072769165,\n",
       "  -7.303987503051758,\n",
       "  -7.004894256591797,\n",
       "  -7.425973892211914,\n",
       "  -7.446069240570068,\n",
       "  -7.661480903625488,\n",
       "  -7.563302516937256,\n",
       "  -7.610910892486572,\n",
       "  -7.497640609741211,\n",
       "  -7.34702730178833,\n",
       "  -6.827414512634277,\n",
       "  -6.945228576660156,\n",
       "  -7.450634479522705,\n",
       "  -7.440974712371826,\n",
       "  -7.372271537780762,\n",
       "  -7.297111511230469,\n",
       "  -7.243414878845215,\n",
       "  -7.417812347412109,\n",
       "  -7.425976753234863,\n",
       "  -7.385310649871826,\n",
       "  -7.125479698181152,\n",
       "  -6.958557605743408,\n",
       "  -7.253373622894287,\n",
       "  -7.1484222412109375,\n",
       "  -7.3401594161987305,\n",
       "  -6.975731372833252,\n",
       "  -7.552231788635254,\n",
       "  -7.199228286743164,\n",
       "  -7.141134738922119,\n",
       "  -7.3705573081970215,\n",
       "  -7.286159038543701,\n",
       "  -7.326327800750732,\n",
       "  -7.196709632873535,\n",
       "  -7.3193359375,\n",
       "  -7.184312343597412,\n",
       "  -7.43895959854126,\n",
       "  -7.254330635070801,\n",
       "  -7.056922912597656,\n",
       "  -7.822439670562744,\n",
       "  -7.488849639892578,\n",
       "  -7.218012809753418,\n",
       "  -7.43849515914917,\n",
       "  -7.183832168579102,\n",
       "  -7.376359939575195,\n",
       "  -7.2316460609436035,\n",
       "  -7.252365589141846,\n",
       "  -7.229292392730713,\n",
       "  -7.03492546081543,\n",
       "  -7.267286777496338,\n",
       "  -7.302170276641846,\n",
       "  -7.364595413208008,\n",
       "  -6.930563449859619,\n",
       "  -7.182311058044434,\n",
       "  -7.309003829956055,\n",
       "  -6.78936767578125,\n",
       "  -7.314920902252197,\n",
       "  -7.345309734344482,\n",
       "  -7.471179485321045,\n",
       "  -7.348825931549072,\n",
       "  -7.267341613769531,\n",
       "  -7.08526086807251,\n",
       "  -7.178172588348389,\n",
       "  -7.073084354400635,\n",
       "  -7.484879016876221,\n",
       "  -7.168468952178955,\n",
       "  -7.37240743637085,\n",
       "  -7.394021987915039,\n",
       "  -7.133391857147217,\n",
       "  -7.2018561363220215,\n",
       "  -7.278707504272461,\n",
       "  -7.156747341156006,\n",
       "  -6.868150234222412,\n",
       "  -7.022101879119873,\n",
       "  -7.503898620605469,\n",
       "  -7.408405780792236,\n",
       "  -7.5022664070129395,\n",
       "  -7.408332347869873,\n",
       "  -7.5507683753967285,\n",
       "  -7.117395877838135,\n",
       "  -7.359447479248047,\n",
       "  -7.405501842498779,\n",
       "  -7.364051818847656,\n",
       "  -6.821287155151367,\n",
       "  -7.070484638214111,\n",
       "  -7.26246452331543,\n",
       "  -7.458441257476807,\n",
       "  -6.9568047523498535,\n",
       "  -7.223861217498779,\n",
       "  -7.295214653015137,\n",
       "  -6.804653167724609,\n",
       "  -7.548985004425049,\n",
       "  -7.367380619049072,\n",
       "  -7.1767497062683105,\n",
       "  -7.208247184753418,\n",
       "  -7.021792411804199,\n",
       "  -7.430655479431152,\n",
       "  -7.254197120666504,\n",
       "  -7.116184234619141,\n",
       "  -7.086449146270752,\n",
       "  -7.408934593200684,\n",
       "  -7.459868907928467,\n",
       "  -6.997673988342285,\n",
       "  -7.157878875732422,\n",
       "  -7.450583457946777,\n",
       "  -7.194190979003906,\n",
       "  -6.889890193939209,\n",
       "  -7.505055904388428,\n",
       "  -7.495627403259277,\n",
       "  -6.986398220062256,\n",
       "  -7.0131425857543945,\n",
       "  -6.757006645202637,\n",
       "  -7.256371974945068,\n",
       "  -7.49597692489624,\n",
       "  -7.636307716369629,\n",
       "  -7.382132530212402,\n",
       "  -6.870404243469238,\n",
       "  -7.194546699523926,\n",
       "  -7.375319004058838,\n",
       "  -7.189782619476318,\n",
       "  -7.21403169631958,\n",
       "  -7.221072196960449,\n",
       "  -7.3337578773498535,\n",
       "  -7.394764423370361,\n",
       "  -6.887303829193115,\n",
       "  -7.3634419441223145,\n",
       "  -7.330747127532959,\n",
       "  -7.287530899047852,\n",
       "  -7.059257507324219,\n",
       "  -7.308027267456055,\n",
       "  -7.0550923347473145,\n",
       "  -7.22796106338501,\n",
       "  -7.245265483856201,\n",
       "  -6.839788436889648,\n",
       "  -7.015496730804443,\n",
       "  -7.201197624206543,\n",
       "  -7.2226176261901855,\n",
       "  -7.117307186126709,\n",
       "  -6.9779510498046875,\n",
       "  -6.860913276672363,\n",
       "  -7.058401107788086,\n",
       "  -7.560776233673096,\n",
       "  -7.682147026062012,\n",
       "  -7.425827503204346,\n",
       "  -7.341599941253662,\n",
       "  -7.559813022613525,\n",
       "  -7.219390392303467,\n",
       "  -7.360923767089844,\n",
       "  -7.497861862182617,\n",
       "  -7.58018159866333,\n",
       "  -7.034561634063721,\n",
       "  -7.05573034286499,\n",
       "  -7.193765163421631,\n",
       "  -7.241013526916504,\n",
       "  -7.466751575469971,\n",
       "  -7.613375663757324,\n",
       "  -7.327300071716309,\n",
       "  -7.344826698303223,\n",
       "  -7.347395896911621,\n",
       "  -7.305792808532715,\n",
       "  -6.990385055541992,\n",
       "  -7.212986469268799,\n",
       "  -7.250670433044434,\n",
       "  -6.927994728088379,\n",
       "  -7.428319454193115,\n",
       "  -7.286937713623047,\n",
       "  -7.471364498138428,\n",
       "  -7.397710800170898,\n",
       "  -7.151201248168945,\n",
       "  -7.433715343475342,\n",
       "  -7.6115827560424805,\n",
       "  -7.092955112457275,\n",
       "  -6.826626777648926,\n",
       "  -7.585275650024414,\n",
       "  ...]]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.LogSoftmax(dim=1)\n",
    "softmax(lin2(h_x)).detach().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-6.5213], grad_fn=<MaxBackward0>), tensor([1173]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the index with highest softmax probabilities\n",
    "torch.max(softmax(lin2(h_x)), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-4-train-cbow\"></a>\n",
    "\n",
    "# Now, we train the CBOW model for real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(211, 24)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First we split the data into training and testing.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tokenized_text_train, tokenized_text_test = train_test_split(tokenized_text, test_size=0.1, random_state=42)\n",
    "len(tokenized_text_train), len(tokenized_text_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim, tensor, autograd\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-1ed73ffed42d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# This unsqueeze thing is really a feature/bug... -_-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "embd_size = 100\n",
    "learning_rate = 0.003\n",
    "hidden_size = 100\n",
    "window_size = 2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Initialize the dataset.\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "vocab_size = len(w2v_dataset.vocab)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "model = CBOW(vocab_size, embd_size, window_size, hidden_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epoch_loss = []\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x, y = w2v_io\n",
    "            x = tensor(x).to(device)\n",
    "            y = autograd.Variable(tensor(y, dtype=torch.long)).to(device)\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = model(x)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(float(loss))\n",
    "    # Save model after every epoch.\n",
    "    torch.save(model.state_dict(), 'cbow_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set(rc={'figure.figsize':(12, 8)})\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-4-evaluate-cbow\"></a>\n",
    "\n",
    "# Apply and Evaluate the CBOW Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_text_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-6ee73468b264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Iterate through the test sentences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Extract all the CBOW contexts (X) and targets (Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw2v_io\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_text_test' is not defined"
     ]
    }
   ],
   "source": [
    "from lazyme import color_str\n",
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x'])\n",
    "        y = tensor(w2v_io['y'])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(model(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.24497991967871485\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-4-load-model\"></a>\n",
    "\n",
    "# Go back to the 10th Epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CBOW(\n",
       "    (embeddings): Embedding(1310, 100)\n",
       "    (linear1): Linear(in_features=400, out_features=100, bias=True)\n",
       "    (linear2): Linear(in_features=100, out_features=1310, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_10 = CBOW(vocab_size, embd_size, window_size, hidden_size)\n",
    "model_10 = torch.nn.DataParallel(model_10)\n",
    "model_10.load_state_dict(torch.load('cbow_checkpoint_10.pt'))\n",
    "model_10.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mis\u001b[0m \t\t the problem \u001b[91m______\u001b[0m essentially this\n",
      "\u001b[92messentially\u001b[0m \t problem is \u001b[91mnot\u001b[0m this :\n",
      "\u001b[92mthis\u001b[0m \t\t is essentially \u001b[91mto\u001b[0m : if\n",
      "\u001b[92m:\u001b[0m \t\t essentially this \u001b[91m(\u001b[0m if a\n",
      "\u001b[92mif\u001b[0m \t\t this : \u001b[91mas\u001b[0m a word\n",
      "\u001b[92ma\u001b[0m \t\t : if \u001b[91mand\u001b[0m word (\n",
      "\u001b[92mword\u001b[0m \t\t if a \u001b[91mtwo\u001b[0m ( or\n",
      "\u001b[92m(\u001b[0m \t\t a word \u001b[91m______\u001b[0m or bigram\n",
      "\u001b[92mor\u001b[0m \t\t word ( \u001b[91m1\u001b[0m bigram ,\n",
      "\u001b[92mbigram\u001b[0m \t\t ( or \u001b[91mrandom\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mrandom\u001b[0m , or\n",
      "\u001b[92m<unk>\u001b[0m \t\t , or \u001b[91mby\u001b[0m etc. )\n",
      "\u001b[92mis\u001b[0m \t\t the web \u001b[92mis\u001b[0m a vast\n",
      "\u001b[92ma\u001b[0m \t\t web is \u001b[91mthe\u001b[0m vast re-\n",
      "\u001b[92mvast\u001b[0m \t\t is a \u001b[91msmall\u001b[0m re- source\n",
      "\u001b[92mre-\u001b[0m \t\t a vast \u001b[91mto\u001b[0m source for\n",
      "\u001b[92msource\u001b[0m \t\t vast re- \u001b[91m:\u001b[0m for many\n",
      "\u001b[92mthe\u001b[0m \t\t is that \u001b[92mthe\u001b[0m association is\n",
      "\u001b[92massociation\u001b[0m \t that the \u001b[91mprobability\u001b[0m is random\n",
      "\u001b[92mis\u001b[0m \t\t the association \u001b[91mrandom\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t association is \u001b[91mever\u001b[0m , arbitrary\n",
      "\u001b[92m,\u001b[0m \t\t is random \u001b[91mand\u001b[0m arbitrary ,\n",
      "\u001b[92marbitrary\u001b[0m \t random , \u001b[92marbitrary\u001b[0m , motivated\n",
      "\u001b[92m,\u001b[0m \t\t , arbitrary \u001b[91m______\u001b[0m motivated or\n",
      "\u001b[92mmotivated\u001b[0m \t arbitrary , \u001b[91mand\u001b[0m or pre-\n",
      "\u001b[92m<unk>\u001b[0m \t\t or pre- \u001b[91mare\u001b[0m ( r\n",
      "\u001b[92m,\u001b[0m \t\t ( r \u001b[91mand\u001b[0m a ,\n",
      "\u001b[92m<unk>\u001b[0m \t\t a , \u001b[91mever\u001b[0m , p\n",
      "\u001b[92mtheir\u001b[0m \t\t however , \u001b[91mwhere\u001b[0m methods are\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[91muncorrelated\u001b[0m , from\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91m______\u001b[0m from just\n",
      "\u001b[92mfrom\u001b[0m \t\t example , \u001b[91mas\u001b[0m just those\n",
      "\u001b[92m<unk>\u001b[0m \t\t just those \u001b[91mthe\u001b[0m errors that\n",
      "\u001b[92mthey\u001b[0m \t\t , and \u001b[91mwe\u001b[0m do not\n",
      "\u001b[92mdo\u001b[0m \t\t and they \u001b[92mdo\u001b[0m not wish\n",
      "\u001b[92mnot\u001b[0m \t\t they do \u001b[91mhave\u001b[0m wish to\n",
      "\u001b[92m<unk>\u001b[0m \t\t wish to \u001b[91mbe\u001b[0m any scf\n",
      "\u001b[92mfor\u001b[0m \t\t any scf \u001b[91mthat\u001b[0m which there\n",
      "\u001b[92mwhich\u001b[0m \t\t scf for \u001b[91mthe\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t for which \u001b[91mlanguage\u001b[0m is any\n",
      "\u001b[92mis\u001b[0m \t\t which there \u001b[92mis\u001b[0m any evidence\n",
      "\u001b[92many\u001b[0m \t\t there is \u001b[91mno\u001b[0m evidence as\n",
      "\u001b[92mevidence\u001b[0m \t is any \u001b[91m______\u001b[0m as a\n",
      "\u001b[92mas\u001b[0m \t\t any evidence \u001b[91mis\u001b[0m a true\n",
      "\u001b[92ma\u001b[0m \t\t evidence as \u001b[91mone\u001b[0m true scf\n",
      "\u001b[92mtrue\u001b[0m \t\t as a \u001b[91mwere\u001b[0m scf for\n",
      "\u001b[92mscf\u001b[0m \t\t a true \u001b[91mmethods\u001b[0m for the\n",
      "\u001b[92mfor\u001b[0m \t\t true scf \u001b[91mgiven\u001b[0m the verb\n",
      "\u001b[92mthe\u001b[0m \t\t scf for \u001b[92mthe\u001b[0m verb .\n",
      "\u001b[92m<unk>\u001b[0m \t\t some way \u001b[91m(\u001b[0m out to\n",
      "\u001b[92m<unk>\u001b[0m \t\t that was \u001b[91mthe\u001b[0m from one\n",
      "\u001b[92mwhere\u001b[0m \t\t from one \u001b[91mof\u001b[0m the individual\n",
      "\u001b[92mthe\u001b[0m \t\t one where \u001b[91mand\u001b[0m individual words\n",
      "\u001b[92mindividual\u001b[0m \t where the \u001b[91mwas\u001b[0m words (\n",
      "\u001b[92mwords\u001b[0m \t\t the individual \u001b[91mfrequency\u001b[0m ( as\n",
      "\u001b[92m(\u001b[0m \t\t individual words \u001b[91mor\u001b[0m as opposed\n",
      "\u001b[92mas\u001b[0m \t\t words ( \u001b[92mas\u001b[0m opposed to\n",
      "\u001b[92mopposed\u001b[0m \t ( as \u001b[91m)\u001b[0m to the\n",
      "\u001b[92mto\u001b[0m \t\t as opposed \u001b[92mto\u001b[0m the texts\n",
      "\u001b[92mthe\u001b[0m \t\t opposed to \u001b[91m(\u001b[0m texts )\n",
      "\u001b[92mtexts\u001b[0m \t\t to the \u001b[91mtwo\u001b[0m ) had\n",
      "\u001b[92m)\u001b[0m \t\t the texts \u001b[91mthey\u001b[0m had been\n",
      "\u001b[92mhad\u001b[0m \t\t texts ) \u001b[91mhas\u001b[0m been randomly\n",
      "\u001b[92mbeen\u001b[0m \t\t ) had \u001b[92mbeen\u001b[0m randomly selected\n",
      "\u001b[92mrandomly\u001b[0m \t had been \u001b[91m1\u001b[0m selected ,\n",
      "\u001b[92mselected\u001b[0m \t been randomly \u001b[91mcorpora\u001b[0m , this\n",
      "\u001b[92m<unk>\u001b[0m \t\t , this \u001b[91mand\u001b[0m out not\n",
      "\u001b[92mto\u001b[0m \t\t out not \u001b[92mto\u001b[0m be the\n",
      "\u001b[92mbe\u001b[0m \t\t not to \u001b[91mreject\u001b[0m the case\n",
      "\u001b[92mthe\u001b[0m \t\t to be \u001b[91mwhere\u001b[0m case .\n",
      "\u001b[92m<unk>\u001b[0m \t\t ted and \u001b[91mthe\u001b[0m carroll 1997\n",
      "\u001b[92m<unk>\u001b[0m \t\t 1997 automatic \u001b[91meach\u001b[0m of subcategorization\n",
      "\u001b[92mfrom\u001b[0m \t\t of subcategorization \u001b[91mlarge\u001b[0m corpora .\n",
      "\u001b[92mwere\u001b[0m \t\t the ho \u001b[91mbetween\u001b[0m tested using\n",
      "\u001b[92mtested\u001b[0m \t\t ho were \u001b[91m)\u001b[0m using the\n",
      "\u001b[92m<unk>\u001b[0m \t\t using the \u001b[91mfiltering\u001b[0m : is\n",
      "\u001b[92m<unk>\u001b[0m \t\t is χ2 \u001b[91mto\u001b[0m &#124; o\n",
      "\u001b[92m⫺\u001b[0m \t\t &#124; o \u001b[91m(\u001b[0m e &#124;\n",
      "\u001b[92me\u001b[0m \t\t o ⫺ \u001b[91mto\u001b[0m &#124; ⫺\n",
      "\u001b[92m&#124;\u001b[0m \t\t ⫺ e \u001b[91m______\u001b[0m ⫺ 0.5\n",
      "\u001b[92m⫺\u001b[0m \t\t e &#124; \u001b[91m______\u001b[0m 0.5 )\n",
      "\u001b[92m0.5\u001b[0m \t\t &#124; ⫺ \u001b[91my\u001b[0m ) 2\n",
      "\u001b[92m)\u001b[0m \t\t ⫺ 0.5 \u001b[91mand\u001b[0m 2 /\n",
      "\u001b[92m2\u001b[0m \t\t 0.5 ) \u001b[91m______\u001b[0m / e\n",
      "\u001b[92m/\u001b[0m \t\t ) 2 \u001b[91m)\u001b[0m e greater\n",
      "\u001b[92me\u001b[0m \t\t 2 / \u001b[91mor\u001b[0m greater than\n",
      "\u001b[92mgreater\u001b[0m \t / e \u001b[91m______\u001b[0m than the\n",
      "\u001b[92mthan\u001b[0m \t\t e greater \u001b[92mthan\u001b[0m the critical\n",
      "\u001b[92mthe\u001b[0m \t\t greater than \u001b[91m0.5\u001b[0m critical value\n",
      "\u001b[92mcritical\u001b[0m \t than the \u001b[91mtwo\u001b[0m value ?\n",
      "\u001b[92m<unk>\u001b[0m \t\t schütze 1999 \u001b[91mof\u001b[0m of statistical\n",
      "\u001b[92mnatural\u001b[0m \t of statistical \u001b[91mand\u001b[0m language processing\n",
      "\u001b[92mlanguage\u001b[0m \t statistical natural \u001b[92mlanguage\u001b[0m processing .\n",
      "\u001b[92mlikelihood\u001b[0m \t if the \u001b[91mlanguage\u001b[0m is low\n",
      "\u001b[92mis\u001b[0m \t\t the likelihood \u001b[91mof\u001b[0m low ,\n",
      "\u001b[92mlow\u001b[0m \t\t likelihood is \u001b[91mcorpora\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t is low \u001b[91m:\u001b[0m we reject\n",
      "\u001b[92mwe\u001b[0m \t\t low , \u001b[91mwith\u001b[0m reject h0\n",
      "\u001b[92mreject\u001b[0m \t\t , we \u001b[91mshall\u001b[0m h0 .\n",
      "\u001b[92mthe\u001b[0m \t\t however where \u001b[92mthe\u001b[0m sample size\n",
      "\u001b[92m<unk>\u001b[0m \t\t sample size \u001b[91mto\u001b[0m by an\n",
      "\u001b[92m<unk>\u001b[0m \t\t order of \u001b[91mrandomness\u001b[0m , or\n",
      "\u001b[92mwhere\u001b[0m \t\t , or \u001b[91mif\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t or where \u001b[91mthere\u001b[0m is enormous\n",
      "\u001b[92mis\u001b[0m \t\t where it \u001b[91mnot\u001b[0m enormous ,\n",
      "\u001b[92menormous\u001b[0m \t it is \u001b[91mnot\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t is enormous \u001b[91mthe\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t enormous , \u001b[91mand\u001b[0m is wrong\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[92mis\u001b[0m wrong to\n",
      "\u001b[92mwrong\u001b[0m \t\t it is \u001b[91mnot\u001b[0m to identify\n",
      "\u001b[92mto\u001b[0m \t\t is wrong \u001b[92mto\u001b[0m identify the\n",
      "\u001b[92m<unk>\u001b[0m \t\t identify the \u001b[91mthe\u001b[0m distinction with\n",
      "\u001b[92m<unk>\u001b[0m \t\t with the \u001b[91mfollowing\u001b[0m one .\n",
      "\u001b[92mthe\u001b[0m \t\t proceedings of \u001b[92mthe\u001b[0m conference of\n",
      "\u001b[92mconference\u001b[0m \t of the \u001b[91mprobability\u001b[0m of the\n",
      "\u001b[92mis\u001b[0m \t\t false assumptions \u001b[91mto\u001b[0m often an\n",
      "\u001b[92m<unk>\u001b[0m \t\t often an \u001b[91mis\u001b[0m way to\n",
      "\u001b[92m<unk>\u001b[0m \t\t way to \u001b[91msee\u001b[0m ; the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the problem \u001b[91m______\u001b[0m where the\n",
      "\u001b[92m<unk>\u001b[0m \t\t of the \u001b[91mdata\u001b[0m is overlooked\n",
      "\u001b[92m<unk>\u001b[0m \t\t tational linguistics \u001b[91m19(\u001b[0m 1 )\n",
      "\u001b[92mis\u001b[0m \t\t conclusion language \u001b[91muse\u001b[0m non-random and\n",
      "\u001b[92mnon-random\u001b[0m \t language is \u001b[91mnot\u001b[0m and hence\n",
      "\u001b[92mand\u001b[0m \t\t is non-random \u001b[91mby\u001b[0m hence ,\n",
      "\u001b[92mhence\u001b[0m \t\t non-random and \u001b[91mdata\u001b[0m , when\n",
      "\u001b[92m,\u001b[0m \t\t and hence \u001b[91mfor\u001b[0m when we\n",
      "\u001b[92mwhen\u001b[0m \t\t hence , \u001b[92mwhen\u001b[0m we look\n",
      "\u001b[92mwe\u001b[0m \t\t , when \u001b[92mwe\u001b[0m look at\n",
      "\u001b[92mlook\u001b[0m \t\t when we \u001b[92mlook\u001b[0m at linguistic\n",
      "\u001b[92m,\u001b[0m \t\t in corpora \u001b[91mand\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t corpora , \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t , the \u001b[92mnull\u001b[0m hypothesis will\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m will never\n",
      "\u001b[92mwill\u001b[0m \t\t null hypothesis \u001b[92mwill\u001b[0m never be\n",
      "\u001b[92mnever\u001b[0m \t\t hypothesis will \u001b[92mnever\u001b[0m be true\n",
      "\u001b[92mbe\u001b[0m \t\t will never \u001b[92mbe\u001b[0m true .\n",
      "\u001b[92mnot\u001b[0m \t\t we do \u001b[92mnot\u001b[0m always have\n",
      "\u001b[92malways\u001b[0m \t\t do not \u001b[91mto\u001b[0m have enough\n",
      "\u001b[92mhave\u001b[0m \t\t not always \u001b[91mthe\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t always have \u001b[92menough\u001b[0m data to\n",
      "\u001b[92mdata\u001b[0m \t\t have enough \u001b[92mdata\u001b[0m to reject\n",
      "\u001b[92mto\u001b[0m \t\t enough data \u001b[92mto\u001b[0m reject the\n",
      "\u001b[92mreject\u001b[0m \t\t data to \u001b[92mreject\u001b[0m the null\n",
      "\u001b[92mthe\u001b[0m \t\t to reject \u001b[92mthe\u001b[0m null hypothesis\n",
      "\u001b[92mnull\u001b[0m \t\t reject the \u001b[92mnull\u001b[0m hypothesis ,\n",
      "\u001b[92mhypothesis\u001b[0m \t the null \u001b[92mhypothesis\u001b[0m , but\n",
      "\u001b[92m,\u001b[0m \t\t null hypothesis \u001b[91mis\u001b[0m but that\n",
      "\u001b[92mbut\u001b[0m \t\t hypothesis , \u001b[91mthe\u001b[0m that is\n",
      "\u001b[92mthat\u001b[0m \t\t , but \u001b[91mlanguage\u001b[0m is a\n",
      "\u001b[92m<unk>\u001b[0m \t\t is a \u001b[91msmall\u001b[0m issue :\n",
      "\u001b[92mwherever\u001b[0m \t issue : \u001b[91mwhere\u001b[0m there is\n",
      "\u001b[92mthere\u001b[0m \t\t : wherever \u001b[92mthere\u001b[0m is enough\n",
      "\u001b[92mis\u001b[0m \t\t wherever there \u001b[92mis\u001b[0m enough data\n",
      "\u001b[92menough\u001b[0m \t\t there is \u001b[92menough\u001b[0m data ,\n",
      "\u001b[92mdata\u001b[0m \t\t is enough \u001b[92mdata\u001b[0m , it\n",
      "\u001b[92m,\u001b[0m \t\t enough data \u001b[91m______\u001b[0m it is\n",
      "\u001b[92mit\u001b[0m \t\t data , \u001b[91mh0\u001b[0m is rejected\n",
      "\u001b[92mis\u001b[0m \t\t , it \u001b[91mset\u001b[0m rejected .\n",
      "\u001b[92min\u001b[0m \t\t since words \u001b[91mare\u001b[0m a text\n",
      "\u001b[92ma\u001b[0m \t\t words in \u001b[91m:\u001b[0m text are\n",
      "\u001b[92mtext\u001b[0m \t\t in a \u001b[91mfrequency\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t a text \u001b[91mdoes\u001b[0m not random\n",
      "\u001b[92mnot\u001b[0m \t\t text are \u001b[91mof\u001b[0m random ,\n",
      "\u001b[92mrandom\u001b[0m \t\t are not \u001b[91mby\u001b[0m , we\n",
      "\u001b[92m,\u001b[0m \t\t not random \u001b[91m______\u001b[0m we know\n",
      "\u001b[92mwe\u001b[0m \t\t random , \u001b[91mand\u001b[0m know that\n",
      "\u001b[92mknow\u001b[0m \t\t , we \u001b[91mcan\u001b[0m that our\n",
      "\u001b[92mthat\u001b[0m \t\t we know \u001b[91mat\u001b[0m our corpora\n",
      "\u001b[92mour\u001b[0m \t\t know that \u001b[91mand\u001b[0m corpora are\n",
      "\u001b[92mcorpora\u001b[0m \t that our \u001b[91mwords\u001b[0m are not\n",
      "\u001b[92mare\u001b[0m \t\t our corpora \u001b[92mare\u001b[0m not randomly\n",
      "\u001b[92mnot\u001b[0m \t\t corpora are \u001b[91mthe\u001b[0m randomly generated\n",
      "\u001b[92mrandomly\u001b[0m \t are not \u001b[91m______\u001b[0m generated ,\n",
      "\u001b[92mgenerated\u001b[0m \t not randomly \u001b[91mcorpora\u001b[0m , and\n",
      "\u001b[92m,\u001b[0m \t\t randomly generated \u001b[91mrandom\u001b[0m and the\n",
      "\u001b[92mand\u001b[0m \t\t generated , \u001b[91mwith\u001b[0m the hypothesis\n",
      "\u001b[92mthe\u001b[0m \t\t , and \u001b[92mthe\u001b[0m hypothesis test\n",
      "\u001b[92mhypothesis\u001b[0m \t and the \u001b[91mdata\u001b[0m test con-\n",
      "\u001b[92m<unk>\u001b[0m \t\t test con- \u001b[91mfor\u001b[0m the fact\n",
      "\u001b[92m<unk>\u001b[0m \t\t cases are \u001b[91mdifferent\u001b[0m in section\n",
      "\u001b[92m<unk>\u001b[0m \t\t of linguistic \u001b[91mχ2\u001b[0m concern the\n",
      "\u001b[92m<unk>\u001b[0m \t\t the dis- \u001b[91mis\u001b[0m between a\n",
      "\u001b[92m<unk>\u001b[0m \t\t a and \u001b[91mwas\u001b[0m a linguistic\n",
      "\u001b[92m<unk>\u001b[0m \t\t a linguistic \u001b[91meach\u001b[0m of a\n",
      "\u001b[92m<unk>\u001b[0m \t\t reason to \u001b[91mas\u001b[0m the relation\n",
      "\u001b[92mbetween\u001b[0m \t the relation \u001b[91msize\u001b[0m , for\n",
      "\u001b[92m,\u001b[0m \t\t relation between \u001b[91m______\u001b[0m for example\n",
      "\u001b[92mfor\u001b[0m \t\t between , \u001b[91mand\u001b[0m example ,\n",
      "\u001b[92mexample\u001b[0m \t , for \u001b[91meach\u001b[0m , a\n",
      "\u001b[92m,\u001b[0m \t\t for example \u001b[91mall\u001b[0m a verb\n",
      "\u001b[92ma\u001b[0m \t\t example , \u001b[91mas\u001b[0m verb ’\n",
      "\u001b[92mverb\u001b[0m \t\t , a \u001b[91mat\u001b[0m ’ s\n",
      "\u001b[92m’\u001b[0m \t\t a verb \u001b[92m’\u001b[0m s syntax\n",
      "\u001b[92ms\u001b[0m \t\t verb ’ \u001b[92ms\u001b[0m syntax and\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92msyntax\u001b[0m \t\t ’ s \u001b[91m______\u001b[0m and its\n",
      "\u001b[92m<unk>\u001b[0m \t\t and its \u001b[91mdata\u001b[0m , as\n",
      "\u001b[92mmotivated\u001b[0m \t , as \u001b[91ma\u001b[0m rather than\n",
      "\u001b[92mrather\u001b[0m \t\t as motivated \u001b[91mless\u001b[0m than arbitrary\n",
      "\u001b[92mthan\u001b[0m \t\t motivated rather \u001b[91mand\u001b[0m arbitrary .\n",
      "\u001b[92mvalue\u001b[0m \t\t the average \u001b[92mvalue\u001b[0m of the\n",
      "\u001b[92mof\u001b[0m \t\t average value \u001b[92mof\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t value of \u001b[92mthe\u001b[0m error term\n",
      "\u001b[92merror\u001b[0m \t\t of the \u001b[91mnull\u001b[0m term ,\n",
      "\u001b[92mterm\u001b[0m \t\t the error \u001b[92mterm\u001b[0m , language\n",
      "\u001b[92m,\u001b[0m \t\t error term \u001b[91mis\u001b[0m language is\n",
      "\u001b[92mlanguage\u001b[0m \t term , \u001b[92mlanguage\u001b[0m is never\n",
      "\u001b[92mis\u001b[0m \t\t , language \u001b[92mis\u001b[0m never ,\n",
      "\u001b[92mnever\u001b[0m \t\t language is \u001b[92mnever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t is never \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t never , \u001b[92mever\u001b[0m , ever\n",
      "\u001b[92m,\u001b[0m \t\t , ever \u001b[91m______\u001b[0m ever ,\n",
      "\u001b[92mever\u001b[0m \t\t ever , \u001b[92mever\u001b[0m , random\n",
      "\u001b[92m<unk>\u001b[0m \t\t , random \u001b[91msamples\u001b[0m ( &#124;\n",
      "\u001b[92mo\u001b[0m \t\t ( &#124; \u001b[91m)\u001b[0m ⫺ e\n",
      "\u001b[92m⫺\u001b[0m \t\t &#124; o \u001b[91m(\u001b[0m e &#124;\n",
      "\u001b[92me\u001b[0m \t\t o ⫺ \u001b[91mto\u001b[0m &#124; ⫺\n",
      "\u001b[92m&#124;\u001b[0m \t\t ⫺ e \u001b[91m______\u001b[0m ⫺ 0.5\n",
      "\u001b[92m⫺\u001b[0m \t\t e &#124; \u001b[91m______\u001b[0m 0.5 )\n",
      "\u001b[92m0.5\u001b[0m \t\t &#124; ⫺ \u001b[91my\u001b[0m ) 2\n",
      "\u001b[92m)\u001b[0m \t\t ⫺ 0.5 \u001b[91mand\u001b[0m 2 /\n",
      "\u001b[92m2\u001b[0m \t\t 0.5 ) \u001b[91m______\u001b[0m / e\n",
      "\u001b[92m/\u001b[0m \t\t ) 2 \u001b[91mthis\u001b[0m e is\n",
      "\u001b[92me\u001b[0m \t\t 2 / \u001b[91mlanguage\u001b[0m is then\n",
      "\u001b[92m<unk>\u001b[0m \t\t is then \u001b[91mof\u001b[0m the hypothesis\n",
      "\u001b[92m<unk>\u001b[0m \t\t can , \u001b[91mwe\u001b[0m , be\n",
      "\u001b[92m<unk>\u001b[0m \t\t , be \u001b[91mable\u001b[0m as :\n",
      "\u001b[92mare\u001b[0m \t\t as : \u001b[91mof\u001b[0m the error\n",
      "\u001b[92mthe\u001b[0m \t\t : are \u001b[91mhigh\u001b[0m error terms\n",
      "\u001b[92merror\u001b[0m \t\t are the \u001b[92merror\u001b[0m terms systematically\n",
      "\u001b[92mterms\u001b[0m \t\t the error \u001b[91mterm\u001b[0m systematically greater\n",
      "\u001b[92msystematically\u001b[0m \t error terms \u001b[91mfar\u001b[0m greater than\n",
      "\u001b[92mgreater\u001b[0m \t terms systematically \u001b[92mgreater\u001b[0m than 0.5\n",
      "\u001b[92mthan\u001b[0m \t\t systematically greater \u001b[92mthan\u001b[0m 0.5 ?\n",
      "\u001b[92m1\u001b[0m \t\t with just \u001b[91mthe\u001b[0m % of\n",
      "\u001b[92m%\u001b[0m \t\t just 1 \u001b[91mand\u001b[0m of them\n",
      "\u001b[92mof\u001b[0m \t\t 1 % \u001b[91mlanguage\u001b[0m them ,\n",
      "\u001b[92mthem\u001b[0m \t\t % of \u001b[91mcorpora\u001b[0m , devastate\n",
      "\u001b[92m<unk>\u001b[0m \t\t , devastate \u001b[91mthis\u001b[0m one of\n",
      "\u001b[92mthe\u001b[0m \t\t one of \u001b[91mthese\u001b[0m verbs for\n",
      "\u001b[92mverbs\u001b[0m \t\t of the \u001b[91mtwo\u001b[0m for which\n",
      "\u001b[92mfor\u001b[0m \t\t the verbs \u001b[91m______\u001b[0m which we\n",
      "\u001b[92mwhich\u001b[0m \t\t verbs for \u001b[91meach\u001b[0m we have\n",
      "\u001b[92m<unk>\u001b[0m \t\t we have \u001b[91mmost\u001b[0m of data\n",
      "\u001b[92m<unk>\u001b[0m \t\t , and \u001b[91mtend\u001b[0m thresholding methods\n",
      "\u001b[92mwill\u001b[0m \t\t thresholding methods \u001b[91mfor\u001b[0m distinguish associated\n",
      "\u001b[92mdistinguish\u001b[0m \t methods will \u001b[91m______\u001b[0m associated scfs\n",
      "\u001b[92massociated\u001b[0m \t will distinguish \u001b[91mand\u001b[0m scfs from\n",
      "\u001b[92mscfs\u001b[0m \t\t distinguish associated \u001b[91m______\u001b[0m from noise\n",
      "\u001b[92mfrom\u001b[0m \t\t associated scfs \u001b[91mwas\u001b[0m noise .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x'])\n",
    "        y = tensor(w2v_io['y'])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(model_10(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.24096385542168675\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [optional] How to Handle Unknown Words? \n",
    "\n",
    "This is not the best way to handle unknown words, but we can simply assign an index for unknown words.\n",
    "\n",
    "**Hint:** Ensure that you have `gensim` version 3.7.0 first. Otherwise this part of the code won't work. Try `python -m pip install -U pip` and then `python -m pip install -U gensim==3.7.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'a', 1: 'bar', 2: 'foo', 3: 'is', 4: 'sentence', 5: 'this'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{6: 'a',\n",
       " 7: 'bar',\n",
       " 2: 'foo',\n",
       " 3: 'is',\n",
       " 4: 'sentence',\n",
       " 5: 'this',\n",
       " 0: '<pad>',\n",
       " 1: '<unk>'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.patch_with_special_tokens\n",
    "vocab = Dictionary(['this is a foo bar sentence'.split()])\n",
    "\n",
    "try:\n",
    "    special_tokens = {'<pad>': 0, '<unk>': 1}\n",
    "    vocab.patch_with_special_tokens(special_tokens)\n",
    "except: # If gensim is not 3.7.0\n",
    "    pass\n",
    "    \n",
    "dict(vocab.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [optional] Lets Rewrite the `Word2VecText` Object\n",
    "\n",
    "Now with the (i) unknown word patch in the vocabulary as well as (ii) `skipgram_iterator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        \n",
    "        # Add the unknown word patch here.\n",
    "        self.vocab = Dictionary(self.sents)\n",
    "        try:\n",
    "            special_tokens = {'<pad>': 0, '<unk>': 1}\n",
    "            self.vocab.patch_with_special_tokens(special_tokens)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = self.cbow_iterator\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = self.skipgram_iterator\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent, self.window_size))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens, unknown_word_index=1)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "    \n",
    "    def cbow_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
    "            \n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            focus = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield {'x': (focus, context_word), 'y':1}\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                if leftovers:\n",
    "                    yield {'x': (focus, random.choice(leftovers)), 'y':0}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-5\"></a>\n",
    "\n",
    "# Lets try the skipgram task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, vocab_size, embd_size):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_context = self.embeddings(context).view((1, -1))\n",
    "        # See https://pytorch.org/docs/stable/torch.html#torch.t\n",
    "        score = torch.mm(embed_focus, torch.t(embed_context))\n",
    "        log_probs = F.logsigmoid(score)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-5-foward\"></a>\n",
    "\n",
    "# Take a closer look at what's in the `forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx1 = torch.rand(1,20)\n",
    "xx2 = torch.rand(1,20)\n",
    "\n",
    "xx1_numpy = xx1.detach().numpy()\n",
    "xx2_numpy = xx2.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 20)\n",
      "(20, 1)\n",
      "[[3.1778643]]\n"
     ]
    }
   ],
   "source": [
    "print(xx1_numpy.shape)\n",
    "print(xx2_numpy.T.shape)\n",
    "print(np.dot(xx1_numpy, xx2_numpy.T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "torch.Size([20, 1])\n",
      "tensor([[3.1779]])\n"
     ]
    }
   ],
   "source": [
    "print(xx1.shape)\n",
    "print(torch.t(xx2).shape) \n",
    "\n",
    "print(torch.mm(xx1, torch.t(xx2))) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-5-train\"></a>\n",
    "\n",
    "# Train a Skipgram model (for real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\n",
      "\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-136-35cae10f640a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# This unsqueeze thing is really a feature/bug... -_-\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogprobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mepcoh_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "embd_size = 100\n",
    "learning_rate = 0.03\n",
    "hidden_size = 300\n",
    "window_size = 3\n",
    "\n",
    "# Initialize the dataset.\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=3, variant='skipgram')\n",
    "vocab_size = len(w2v_dataset.vocab)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "model = SkipGram(vocab_size, embd_size,).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epcoh_loss = 0\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x1, x2 = w2v_io['x']\n",
    "            x1, x2 = tensor(x1).to(device), tensor(x2).to(device)\n",
    "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.float)).to(device)\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = model(x1, x2)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epcoh_loss += float(loss)\n",
    "    torch.save(model.state_dict(), 'skipgram_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(epcoh_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-5-evaluate\"></a>\n",
    "\n",
    "# Evaluate the model on the skipgram task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "skipgram_iterator() missing 1 required positional argument: 'window_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-687508993e2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Extract all the CBOW contexts (X) and targets (Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mw2v_io\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Retrieve the inputs and outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: skipgram_iterator() missing 1 required positional argument: 'window_size'"
     ]
    }
   ],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        model.zero_grad()\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x1, x2 = w2v_io['x']\n",
    "        x1, x2 = tensor(x1), tensor(x2)\n",
    "        y = w2v_io['y']\n",
    "        _, prediction =  torch.max(model(x1, x2), 1)    \n",
    "        true_positive += int(prediction) == int(y)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-a1fbc309b646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_positive\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the Collobert and Weston SENNA Embeddings\n",
    "\n",
    "\n",
    "If you're on a Mac or Linux, you can use the `!` bang commands in the next cell to get the data.\n",
    "\n",
    "```\n",
    "!pip install kaggle\n",
    "!mkdir -p .kaggle\n",
    "!echo '{\"username\":\"natgillin\",\"key\":\"54ae95ab760b52c3307ed4645c6c9b5d\"}' > .kaggle/kaggle.json\n",
    "!chmod 600 .kaggle/kaggle.json\n",
    "!kaggle datasets download -d alvations/vegetables-senna-embeddings --force -p ./\n",
    "```\n",
    "\n",
    "If you're on windows go to https://www.kaggle.com/alvations/vegetables-senna-embeddings and download the data files. \n",
    "\n",
    "What's most important are the \n",
    " - `.txt` file that contains the vocabulary list\n",
    " - `.npy` file that contains the binarized numpy array\n",
    " \n",
    "The rows of the numpy array corresponds to the vocabulary in the order from the `.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-vocab\"></a>\n",
    "\n",
    "\n",
    "## 3.1.6. Loading Pre-trained Embeddings\n",
    "\n",
    "Lets overwrite the `Word2VecText` object with the pretrained embeddings. \n",
    "\n",
    "Most important thing is the overwrite the `Dictionary` from `gensim` with the vocabulary of the pre-trained embeddings, as such:\n",
    "\n",
    "```python\n",
    "        # Loads the pretrained keys. \n",
    "        with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "            pretrained_keys = {line.strip():i for i, line in enumerate(fin)}\n",
    "        self.vocab = Dictionary({})\n",
    "        self.vocab.token2id = pretrained_keys\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecText(Dataset):\n",
    "    def __init__(self, tokenized_texts, window_size, variant):\n",
    "        \"\"\"\n",
    "        :param tokenized_texts: Tokenized text.\n",
    "        :type tokenized_texts: list(list(str))\n",
    "        \"\"\"\n",
    "        self.sents = tokenized_texts\n",
    "        self._len = len(self.sents)\n",
    "        \n",
    "        # Loads the pretrained keys. \n",
    "        with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "            pretrained_keys = {line.strip():i for i, line in enumerate(fin)}\n",
    "        self.vocab = Dictionary({})\n",
    "        self.vocab.token2id = pretrained_keys\n",
    "        \n",
    "        self.window_size = window_size\n",
    "        self.variant = variant\n",
    "        if variant.lower() == 'cbow':\n",
    "            self._iterator = partial(self.cbow_iterator, window_size=self.window_size)\n",
    "        elif variant.lower() == 'skipgram':\n",
    "            self._iterator = partial(self.skipgram_iterator, window_size=self.window_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        The primary entry point for PyTorch datasets.\n",
    "        This is were you access the specific data row you want.\n",
    "        \n",
    "        :param index: Index to the data point.\n",
    "        :type index: int\n",
    "        \"\"\"\n",
    "        vectorized_sent = self.vectorize(self.sents[index])\n",
    "        \n",
    "        return list(self._iterator(vectorized_sent))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "    \n",
    "    def vectorize(self, tokens):\n",
    "        \"\"\"\n",
    "        :param tokens: Tokens that should be vectorized. \n",
    "        :type tokens: list(str)\n",
    "        \"\"\"\n",
    "        # See https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2idx \n",
    "        return self.vocab.doc2idx(tokens, unknown_word_index=-1)\n",
    "    \n",
    "    def unvectorize(self, indices):\n",
    "        \"\"\"\n",
    "        :param indices: Converts the indices back to tokens.\n",
    "        :type tokens: list(int)\n",
    "        \"\"\"\n",
    "        return [self.vocab[i] for i in indices]\n",
    "    \n",
    "    def cbow_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1\n",
    "        for window in per_window(tokens, n):\n",
    "            target = window.pop(window_size)\n",
    "            yield {'x': window, 'y': target}   # X = window ; Y = target. \n",
    "            \n",
    "    def skipgram_iterator(self, tokens, window_size):\n",
    "        n = window_size * 2 + 1 \n",
    "        for i, window in enumerate(per_window(tokens, n)):\n",
    "            focus = window.pop(window_size)\n",
    "            # Generate positive samples.\n",
    "            for context_word in window:\n",
    "                yield {'x': (focus, context_word), 'y':1}\n",
    "            # Generate negative samples.\n",
    "            for _ in range(n-1):\n",
    "                leftovers = tokens[:i] + tokens[i+n:]\n",
    "                if leftovers:\n",
    "                    yield {'x': (focus, random.choice(leftovers)), 'y':0}\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-pretrained\"></a>\n",
    "\n",
    "## Override the embeddings layer with the pre-trained weights.\n",
    "\n",
    "In PyTorch, the weights of the `nn.Embedding` object can be easily overwritten with `from_pretrained` function, see https://pytorch.org/docs/stable/nn.html#embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "    def __init__(self, pretrained_npy):\n",
    "        super(SkipGram, self).__init__()\n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy)\n",
    "    \n",
    "    def forward(self, focus, context):\n",
    "        embed_focus = self.embeddings(focus).view((1, -1))\n",
    "        embed_context = self.embeddings(context).view((1, -1))\n",
    "        # See https://pytorch.org/docs/stable/torch.html#torch.t\n",
    "        score = torch.mm(embed_focus, torch.t(embed_context))\n",
    "        log_probs = F.logsigmoid(score)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_text_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-7a6e8133caa4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw2v_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2VecText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skipgram'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpretrained_npy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'senna.wiki-reuters.lm2.50d.npy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpretrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSkipGram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_npy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_text_train' is not defined"
     ]
    }
   ],
   "source": [
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='skipgram')\n",
    "pretrained_npy = torch.tensor(np.load('senna.wiki-reuters.lm2.50d.npy'))\n",
    "pretrained_model = SkipGram(pretrained_npy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-eval-skipgram\"></a>\n",
    "## Test Pretrained Embeddings on the Skipgram Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_text_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-470936aca002>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Iterate through the test sentences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;31m# Extract all the CBOW contexts (X) and targets (Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw2v_io\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_text_test' is not defined"
     ]
    }
   ],
   "source": [
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        pretrained_model.zero_grad()\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x1, x2 = w2v_io['x']\n",
    "        if -1 in (x1, x2): # Skip unknown words.\n",
    "            continue\n",
    "        x1, x2 = tensor(x1), tensor(x2)\n",
    "        y = w2v_io['y']\n",
    "        with torch.no_grad():\n",
    "            logprobs = pretrained_model(x1, x2)\n",
    "            _, prediction =  torch.max(logprobs, 1)    \n",
    "        true_positive += int(prediction) == int(y)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('senna.wiki-reuters.lm2.50d.txt') as fin:\n",
    "    pretrained_keys = {line.strip():i for i, line in enumerate(fin)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-a1fbc309b646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_positive\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-eval-cbow\"></a>\n",
    "## Test Pretrained Embeddings on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, pretrained_npy, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        vocab_size, embd_size = list(pretrained_npy.shape)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).float().view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_text_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-59ff07c52046>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2VecText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cbow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpretrained_cbow_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBOW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_npy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_text_train' is not defined"
     ]
    }
   ],
   "source": [
    "window_size = 5\n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "hidden_size = 300\n",
    "pretrained_cbow_model = CBOW(pretrained_npy, window_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_text_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-078498ff5ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Iterate through the test sentences.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text_test\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Extract all the CBOW contexts (X) and targets (Y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mw2v_io\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2v_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_text_test' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x'])\n",
    "        y = tensor(w2v_io['y'])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(pretrained_cbow_model(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-a1fbc309b646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_positive\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-unfreeze-finetune\"></a>\n",
    "## Unfreeze the Embedddings and Tune it on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "    def __init__(self, pretrained_npy, context_size, hidden_size):\n",
    "        super(CBOW, self).__init__()\n",
    "        vocab_size, embd_size = list(pretrained_npy.shape)\n",
    "        self.embeddings = nn.Embedding.from_pretrained(pretrained_npy, freeze=False)\n",
    "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embeddings(inputs).float().view((1, -1))\n",
    "        hid = F.relu(self.linear1(embedded))\n",
    "        out = self.linear2(hid)\n",
    "        log_probs = F.log_softmax(out, dim=1)\n",
    "        return log_probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_text_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-985992808650>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mw2v_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2VecText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_text_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cbow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpretrained_cbow_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBOW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_npy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenized_text_train' is not defined"
     ]
    }
   ],
   "source": [
    "window_size = 2 \n",
    "w2v_dataset = Word2VecText(tokenized_text_train, window_size=window_size, variant='cbow')\n",
    "hidden_size = 300\n",
    "pretrained_cbow_model = CBOW(pretrained_npy, window_size, hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pretrained_cbow_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-4f84802592bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.003\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_cbow_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pretrained_cbow_model' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.003\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(pretrained_cbow_model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "model = nn.DataParallel(pretrained_cbow_model)\n",
    "\n",
    "num_epochs = 100\n",
    "for _e in tqdm(range(num_epochs)):\n",
    "    epoch_loss = []\n",
    "    for sent_idx in range(w2v_dataset._len):\n",
    "        for w2v_io in w2v_dataset[sent_idx]:\n",
    "            # Retrieve the inputs and outputs.\n",
    "            x = tensor(w2v_io['x'])\n",
    "            y = autograd.Variable(tensor(w2v_io['y'], dtype=torch.long))\n",
    "            \n",
    "            if -1 in x or int(y) == -1:\n",
    "                continue\n",
    "            # Zero gradient.\n",
    "            model.zero_grad()\n",
    "            # Calculate the log probability of the context embeddings.\n",
    "            logprobs = pretrained_cbow_model(x)\n",
    "            # This unsqueeze thing is really a feature/bug... -_-\n",
    "            loss = criterion(logprobs, y.unsqueeze(0)) \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss.append(float(loss))\n",
    "    # Save model after every epoch.\n",
    "    torch.save(model.state_dict(), 'cbow_finetuning_checkpoint_{}.pt'.format(_e))\n",
    "    losses.append(sum(epoch_loss)/len(epoch_loss))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-3-1-6-reval-cbow\"></a>\n",
    "\n",
    "## Re-Test Pretrained Embeddings on the CBOW Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "true_positive = 0\n",
    "all_data = 0\n",
    "# Iterate through the test sentences. \n",
    "for sent in tokenized_text_test:\n",
    "    # Extract all the CBOW contexts (X) and targets (Y)\n",
    "    for w2v_io in w2v_dataset._iterator(w2v_dataset.vectorize(sent)):\n",
    "        # Retrieve the inputs and outputs.\n",
    "        x = tensor(w2v_io['x'])\n",
    "        y = tensor(w2v_io['y'])\n",
    "        \n",
    "        if -1 in x: # Skip unknown words.\n",
    "            continue\n",
    "        with torch.no_grad():\n",
    "            _, prediction =  torch.max(pretrained_cbow_model(x), 1)\n",
    "        true_positive += int(prediction) == int(y)\n",
    "        visualize_predictions(x, y, prediction, w2v_dataset.vocab, window_size=window_size)\n",
    "        all_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-a1fbc309b646>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_positive\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "print('Accuracy:', true_positive/all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
